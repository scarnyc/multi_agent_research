{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Multi-Agent Research System Evaluation Notebook\n",
    "\n",
    "This notebook provides a comprehensive interface for running and evaluating the multi-agent research system with Phoenix observability.\n",
    "\n",
    "## üìã Features\n",
    "- Interactive evaluation controls\n",
    "- Real-time progress tracking\n",
    "- Phoenix tracing integration\n",
    "- Comprehensive result analysis\n",
    "- Quality metrics visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üêç Python path updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import multi-agent system components\n",
    "from agents.multi_agents import MultiAgentResearchSystem, initialize_system\n",
    "from agents.supervisor import SupervisorAgent\n",
    "from agents.search import SearchAgent\n",
    "from agents.citation import CitationAgent\n",
    "from config.settings import settings, ReasoningEffort, Verbosity\n",
    "from evaluation_dataset import EVALUATION_QUERIES, get_queries_by_complexity\n",
    "from evaluation.phoenix_integration import phoenix_integration, start_evaluation_session\n",
    "\n",
    "print(\"‚úÖ Multi-agent system imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging for notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set library loggers to WARNING to reduce noise\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "logging.getLogger('openai').setLevel(logging.WARNING)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "print(\"üìù Logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ System Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the multi-agent research system\n",
    "system = MultiAgentResearchSystem(\n",
    "    supervisor_reasoning=ReasoningEffort.MEDIUM,\n",
    "    supervisor_verbosity=Verbosity.MEDIUM,\n",
    "    search_reasoning=ReasoningEffort.LOW,\n",
    "    citation_reasoning=ReasoningEffort.LOW,\n",
    "    enable_phoenix_tracing=True\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Multi-Agent Research System Initialized\")\n",
    "print(f\"üìä Agents: {list(system.supervisor.sub_agents.keys())}\")\n",
    "print(f\"üîç Phoenix Tracing: {'Enabled' if system.enable_phoenix_tracing else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Phoenix Integration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Phoenix evaluation session\n",
    "session_name = f\"notebook_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "session_id = await start_evaluation_session(session_name)\n",
    "\n",
    "print(f\"üî• Phoenix Session Started: {session_id}\")\n",
    "print(f\"üìç Access Phoenix UI at: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Evaluation Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation dataset summary\n",
    "def analyze_dataset():\n",
    "    \"\"\"Analyze and display evaluation dataset statistics.\"\"\"\n",
    "    total_queries = len(EVALUATION_QUERIES)\n",
    "    \n",
    "    # Group by complexity\n",
    "    complexity_counts = {}\n",
    "    query_types = {}\n",
    "    domains = {}\n",
    "    current_info_needed = 0\n",
    "    \n",
    "    for query in EVALUATION_QUERIES:\n",
    "        # Count by complexity\n",
    "        complexity = query.get('expected_model', 'unknown')\n",
    "        complexity_counts[complexity] = complexity_counts.get(complexity, 0) + 1\n",
    "        \n",
    "        # Count by type\n",
    "        query_type = query.get('type', 'unknown')\n",
    "        query_types[query_type] = query_types.get(query_type, 0) + 1\n",
    "        \n",
    "        # Count by domain\n",
    "        domain = query.get('domain', 'unknown')\n",
    "        domains[domain] = domains.get(domain, 0) + 1\n",
    "        \n",
    "        # Count current info requirements\n",
    "        if query.get('requires_current_info', False):\n",
    "            current_info_needed += 1\n",
    "    \n",
    "    return {\n",
    "        'total': total_queries,\n",
    "        'complexity': complexity_counts,\n",
    "        'types': query_types,\n",
    "        'domains': domains,\n",
    "        'current_info': current_info_needed\n",
    "    }\n",
    "\n",
    "dataset_stats = analyze_dataset()\n",
    "\n",
    "print(\"üìà Evaluation Dataset Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Total Queries: {dataset_stats['total']}\")\n",
    "print(f\"üîç Require Current Info: {dataset_stats['current_info']}\")\n",
    "print()\n",
    "print(\"üìã By Complexity:\")\n",
    "for complexity, count in dataset_stats['complexity'].items():\n",
    "    print(f\"  {complexity}: {count} queries\")\n",
    "print()\n",
    "print(\"üìù By Type:\")\n",
    "for qtype, count in dataset_stats['types'].items():\n",
    "    print(f\"  {qtype}: {count} queries\")\n",
    "print()\n",
    "print(\"üè∑Ô∏è By Domain (top 10):\")\n",
    "sorted_domains = sorted(dataset_stats['domains'].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for domain, count in sorted_domains:\n",
    "    print(f\"  {domain}: {count} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Evaluation Dataset Distribution', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Complexity distribution\n",
    "complexities = list(dataset_stats['complexity'].keys())\n",
    "complexity_counts = list(dataset_stats['complexity'].values())\n",
    "axes[0,0].pie(complexity_counts, labels=complexities, autopct='%1.1f%%', startangle=90)\n",
    "axes[0,0].set_title('Distribution by Complexity')\n",
    "\n",
    "# Query type distribution  \n",
    "types = list(dataset_stats['types'].keys())\n",
    "type_counts = list(dataset_stats['types'].values())\n",
    "axes[0,1].pie(type_counts, labels=types, autopct='%1.1f%%', startangle=90)\n",
    "axes[0,1].set_title('Distribution by Query Type')\n",
    "\n",
    "# Top domains\n",
    "top_domains = sorted_domains[:8]\n",
    "domain_names = [d[0] for d in top_domains]\n",
    "domain_counts = [d[1] for d in top_domains]\n",
    "axes[1,0].bar(range(len(domain_names)), domain_counts)\n",
    "axes[1,0].set_xticks(range(len(domain_names)))\n",
    "axes[1,0].set_xticklabels(domain_names, rotation=45, ha='right')\n",
    "axes[1,0].set_title('Top 8 Domains')\n",
    "axes[1,0].set_ylabel('Number of Queries')\n",
    "\n",
    "# Current info requirement\n",
    "current_info_data = ['Requires Current Info', 'Historical Info OK']\n",
    "current_info_counts = [dataset_stats['current_info'], dataset_stats['total'] - dataset_stats['current_info']]\n",
    "axes[1,1].pie(current_info_counts, labels=current_info_data, autopct='%1.1f%%', startangle=90)\n",
    "axes[1,1].set_title('Current Information Requirements')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Interactive Evaluation Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive widgets for evaluation control\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "# Evaluation control widgets\n",
    "max_queries_slider = widgets.IntSlider(\n",
    "    value=5,\n",
    "    min=1,\n",
    "    max=20,\n",
    "    step=1,\n",
    "    description='Max Queries:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "complexity_filter = widgets.SelectMultiple(\n",
    "    options=['gpt-5-nano', 'gpt-5-mini', 'gpt-5'],\n",
    "    value=['gpt-5-nano', 'gpt-5-mini', 'gpt-5'],\n",
    "    description='Complexity Levels:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "domain_filter = widgets.Dropdown(\n",
    "    options=['All'] + [d[0] for d in sorted_domains[:10]],\n",
    "    value='All',\n",
    "    description='Domain Filter:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='üöÄ Run Evaluation',\n",
    "    button_style='success',\n",
    "    layout={'width': '200px', 'height': '40px'}\n",
    ")\n",
    "\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progress:',\n",
    "    bar_style='info',\n",
    "    style={'bar_color': '#1f77b4', 'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "status_output = widgets.Output()\n",
    "\n",
    "print(\"üéÆ Interactive Evaluation Controls\")\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([max_queries_slider]),\n",
    "    widgets.HBox([complexity_filter]),\n",
    "    widgets.HBox([domain_filter]),\n",
    "    widgets.HBox([run_button]),\n",
    "    widgets.HBox([progress_bar]),\n",
    "    status_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation results storage\n",
    "evaluation_results = []\n",
    "current_evaluation_session = None\n",
    "\n",
    "async def run_single_evaluation(query_data: Dict, session_id: str) -> Dict:\n",
    "    \"\"\"Run evaluation on a single query and collect metrics.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Process query through multi-agent system\n",
    "        result = await system.process_query(\n",
    "            query=query_data['query'],\n",
    "            trace_id=f\"{session_id}_{query_data.get('id', 'unknown')}\",\n",
    "            session_id=session_id\n",
    "        )\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # Extract key metrics\n",
    "        response_length = len(result.get('response', ''))\n",
    "        citations_count = len(result.get('citations', []))\n",
    "        total_tokens = result.get('total_tokens', 0)\n",
    "        \n",
    "        # Simple quality metrics (would be enhanced with proper evaluation models)\n",
    "        quality_score = min(1.0, response_length / 500)  # Basic length-based score\n",
    "        citation_completeness = min(1.0, citations_count / query_data.get('expected_sources', 5))\n",
    "        \n",
    "        return {\n",
    "            'query_id': query_data.get('id', 'unknown'),\n",
    "            'query': query_data['query'],\n",
    "            'expected_complexity': query_data.get('expected_model', 'unknown'),\n",
    "            'domain': query_data.get('domain', 'unknown'),\n",
    "            'type': query_data.get('type', 'unknown'),\n",
    "            'status': result.get('status', 'unknown'),\n",
    "            'response_length': response_length,\n",
    "            'citations_count': citations_count,\n",
    "            'total_tokens': total_tokens,\n",
    "            'execution_time': execution_time,\n",
    "            'quality_score': quality_score,\n",
    "            'citation_completeness': citation_completeness,\n",
    "            'agents_used': result.get('agents_used', []),\n",
    "            'model_used': result.get('model_used', 'unknown'),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'success': result.get('status') == 'success'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        logger.error(f\"Evaluation failed for query {query_data.get('id', 'unknown')}: {str(e)}\")\n",
    "        \n",
    "        return {\n",
    "            'query_id': query_data.get('id', 'unknown'),\n",
    "            'query': query_data['query'],\n",
    "            'expected_complexity': query_data.get('expected_model', 'unknown'),\n",
    "            'domain': query_data.get('domain', 'unknown'),\n",
    "            'type': query_data.get('type', 'unknown'),\n",
    "            'status': 'error',\n",
    "            'response_length': 0,\n",
    "            'citations_count': 0,\n",
    "            'total_tokens': 0,\n",
    "            'execution_time': execution_time,\n",
    "            'quality_score': 0.0,\n",
    "            'citation_completeness': 0.0,\n",
    "            'agents_used': [],\n",
    "            'model_used': 'error',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "async def run_evaluation_batch(queries: List[Dict], session_id: str) -> List[Dict]:\n",
    "    \"\"\"Run evaluation on a batch of queries with progress tracking.\"\"\"\n",
    "    results = []\n",
    "    total = len(queries)\n",
    "    \n",
    "    with status_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"üöÄ Starting evaluation of {total} queries...\")\n",
    "    \n",
    "    for i, query_data in enumerate(queries):\n",
    "        # Update progress\n",
    "        progress_bar.value = int((i / total) * 100)\n",
    "        \n",
    "        with status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"üìù Processing query {i+1}/{total}: {query_data['query'][:50]}...\")\n",
    "        \n",
    "        # Run single evaluation\n",
    "        result = await run_single_evaluation(query_data, session_id)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Brief pause to prevent overwhelming the API\n",
    "        await asyncio.sleep(0.5)\n",
    "    \n",
    "    progress_bar.value = 100\n",
    "    with status_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"‚úÖ Evaluation complete! Processed {len(results)} queries\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"üìä Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Button click handler\n",
    "async def on_run_button_clicked(b):\n",
    "    \"\"\"Handle run evaluation button click.\"\"\"\n",
    "    global evaluation_results, current_evaluation_session\n",
    "    \n",
    "    # Get filter settings\n",
    "    max_queries = max_queries_slider.value\n",
    "    selected_complexities = list(complexity_filter.value)\n",
    "    selected_domain = domain_filter.value\n",
    "    \n",
    "    # Filter queries based on settings\n",
    "    filtered_queries = []\n",
    "    for query in EVALUATION_QUERIES:\n",
    "        # Filter by complexity\n",
    "        if query.get('expected_model', 'unknown') not in selected_complexities:\n",
    "            continue\n",
    "            \n",
    "        # Filter by domain\n",
    "        if selected_domain != 'All' and query.get('domain', 'unknown') != selected_domain:\n",
    "            continue\n",
    "            \n",
    "        filtered_queries.append(query)\n",
    "        \n",
    "        # Limit to max queries\n",
    "        if len(filtered_queries) >= max_queries:\n",
    "            break\n",
    "    \n",
    "    if not filtered_queries:\n",
    "        with status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"‚ö†Ô∏è No queries match the selected filters!\")\n",
    "        return\n",
    "    \n",
    "    # Start new evaluation session\n",
    "    eval_session_id = f\"interactive_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    current_evaluation_session = eval_session_id\n",
    "    \n",
    "    # Run evaluation\n",
    "    try:\n",
    "        run_button.disabled = True\n",
    "        run_button.description = '‚è≥ Running...'\n",
    "        \n",
    "        results = await run_evaluation_batch(filtered_queries, eval_session_id)\n",
    "        evaluation_results.extend(results)\n",
    "        \n",
    "        # Show summary\n",
    "        successful = len([r for r in results if r['success']])\n",
    "        failed = len(results) - successful\n",
    "        avg_time = sum(r['execution_time'] for r in results) / len(results)\n",
    "        total_tokens = sum(r['total_tokens'] for r in results)\n",
    "        \n",
    "        with status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"üéâ Evaluation Complete!\")\n",
    "            print(f\"‚úÖ Successful: {successful}\")\n",
    "            print(f\"‚ùå Failed: {failed}\")\n",
    "            print(f\"‚è±Ô∏è Avg Time: {avg_time:.2f}s\")\n",
    "            print(f\"ü™ô Total Tokens: {total_tokens:,}\")\n",
    "            print(f\"üìä Results stored in evaluation_results\")\n",
    "        \n",
    "    finally:\n",
    "        run_button.disabled = False\n",
    "        run_button.description = 'üöÄ Run Evaluation'\n",
    "\n",
    "# Connect button to handler\n",
    "run_button.on_click(lambda b: asyncio.create_task(on_run_button_clicked(b)))\n",
    "\n",
    "print(\"üéÆ Button handler connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Quick Test Evaluation\n",
    "\n",
    "Let's run a quick test with a few sample queries to verify everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test evaluation\n",
    "test_queries = [\n",
    "    {\n",
    "        'id': 'test_1',\n",
    "        'query': 'What is artificial intelligence?',\n",
    "        'expected_model': 'gpt-5-nano',\n",
    "        'domain': 'technology',\n",
    "        'type': 'qa',\n",
    "        'expected_sources': 3\n",
    "    },\n",
    "    {\n",
    "        'id': 'test_2', \n",
    "        'query': 'Explain the latest developments in quantum computing and their implications for cryptography',\n",
    "        'expected_model': 'gpt-5',\n",
    "        'domain': 'technology',\n",
    "        'type': 'research',\n",
    "        'expected_sources': 5\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üß™ Running quick test evaluation...\")\n",
    "test_session_id = f\"test_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "test_results = await run_evaluation_batch(test_queries, test_session_id)\n",
    "\n",
    "# Display test results\n",
    "print(\"\\nüìä Test Results:\")\n",
    "for result in test_results:\n",
    "    status_emoji = \"‚úÖ\" if result['success'] else \"‚ùå\"\n",
    "    print(f\"{status_emoji} {result['query_id']}: {result['execution_time']:.2f}s, {result['total_tokens']} tokens, {result['citations_count']} citations\")\n",
    "\n",
    "print(f\"\\nüéØ Test completed! {len([r for r in test_results if r['success']])}/{len(test_results)} successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results analysis functions\n",
    "def analyze_evaluation_results(results: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze evaluation results and compute comprehensive metrics.\"\"\"\n",
    "    if not results:\n",
    "        return {\"error\": \"No results to analyze\"}\n",
    "    \n",
    "    successful = [r for r in results if r['success']]\n",
    "    failed = [r for r in results if not r['success']]\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics = {\n",
    "        'total_queries': len(results),\n",
    "        'successful': len(successful),\n",
    "        'failed': len(failed),\n",
    "        'success_rate': len(successful) / len(results) if results else 0,\n",
    "    }\n",
    "    \n",
    "    if successful:\n",
    "        # Performance metrics\n",
    "        execution_times = [r['execution_time'] for r in successful]\n",
    "        token_counts = [r['total_tokens'] for r in successful]\n",
    "        response_lengths = [r['response_length'] for r in successful]\n",
    "        citation_counts = [r['citations_count'] for r in successful]\n",
    "        quality_scores = [r['quality_score'] for r in successful]\n",
    "        \n",
    "        metrics.update({\n",
    "            'avg_execution_time': sum(execution_times) / len(execution_times),\n",
    "            'median_execution_time': sorted(execution_times)[len(execution_times)//2],\n",
    "            'max_execution_time': max(execution_times),\n",
    "            'min_execution_time': min(execution_times),\n",
    "            'total_tokens': sum(token_counts),\n",
    "            'avg_tokens': sum(token_counts) / len(token_counts),\n",
    "            'avg_response_length': sum(response_lengths) / len(response_lengths),\n",
    "            'avg_citations': sum(citation_counts) / len(citation_counts),\n",
    "            'avg_quality_score': sum(quality_scores) / len(quality_scores),\n",
    "        })\n",
    "        \n",
    "        # Performance by complexity\n",
    "        by_complexity = {}\n",
    "        for result in successful:\n",
    "            complexity = result['expected_complexity']\n",
    "            if complexity not in by_complexity:\n",
    "                by_complexity[complexity] = []\n",
    "            by_complexity[complexity].append(result)\n",
    "        \n",
    "        complexity_metrics = {}\n",
    "        for complexity, comp_results in by_complexity.items():\n",
    "            complexity_metrics[complexity] = {\n",
    "                'count': len(comp_results),\n",
    "                'avg_time': sum(r['execution_time'] for r in comp_results) / len(comp_results),\n",
    "                'avg_tokens': sum(r['total_tokens'] for r in comp_results) / len(comp_results),\n",
    "                'avg_quality': sum(r['quality_score'] for r in comp_results) / len(comp_results),\n",
    "            }\n",
    "        \n",
    "        metrics['by_complexity'] = complexity_metrics\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def visualize_results(results: List[Dict]):\n",
    "    \"\"\"Create comprehensive visualizations of evaluation results.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Multi-Agent System Evaluation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Success Rate\n",
    "    success_counts = df['success'].value_counts()\n",
    "    axes[0,0].pie(success_counts.values, labels=['Success', 'Failed'], autopct='%1.1f%%', \n",
    "                  colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
    "    axes[0,0].set_title('Success Rate')\n",
    "    \n",
    "    # 2. Execution Time Distribution\n",
    "    successful_df = df[df['success'] == True]\n",
    "    if not successful_df.empty:\n",
    "        axes[0,1].hist(successful_df['execution_time'], bins=15, alpha=0.7, color='#3498db')\n",
    "        axes[0,1].set_xlabel('Execution Time (seconds)')\n",
    "        axes[0,1].set_ylabel('Frequency')\n",
    "        axes[0,1].set_title('Execution Time Distribution')\n",
    "        axes[0,1].axvline(successful_df['execution_time'].mean(), color='red', linestyle='--', \n",
    "                         label=f'Mean: {successful_df[\"execution_time\"].mean():.2f}s')\n",
    "        axes[0,1].legend()\n",
    "    \n",
    "    # 3. Token Usage by Complexity\n",
    "    if not successful_df.empty:\n",
    "        complexity_tokens = successful_df.groupby('expected_complexity')['total_tokens'].mean()\n",
    "        axes[0,2].bar(complexity_tokens.index, complexity_tokens.values, color='#9b59b6')\n",
    "        axes[0,2].set_xlabel('Complexity Level')\n",
    "        axes[0,2].set_ylabel('Average Tokens')\n",
    "        axes[0,2].set_title('Token Usage by Complexity')\n",
    "        axes[0,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Response Length vs Citations\n",
    "    if not successful_df.empty:\n",
    "        scatter = axes[1,0].scatter(successful_df['response_length'], successful_df['citations_count'], \n",
    "                                   c=successful_df['execution_time'], cmap='viridis', alpha=0.6)\n",
    "        axes[1,0].set_xlabel('Response Length (characters)')\n",
    "        axes[1,0].set_ylabel('Number of Citations')\n",
    "        axes[1,0].set_title('Response Length vs Citations')\n",
    "        plt.colorbar(scatter, ax=axes[1,0], label='Execution Time (s)')\n",
    "    \n",
    "    # 5. Performance by Domain\n",
    "    if not successful_df.empty:\n",
    "        domain_performance = successful_df.groupby('domain')['quality_score'].mean().sort_values(ascending=False)\n",
    "        if len(domain_performance) > 10:\n",
    "            domain_performance = domain_performance.head(10)\n",
    "        axes[1,1].barh(range(len(domain_performance)), domain_performance.values, color='#e67e22')\n",
    "        axes[1,1].set_yticks(range(len(domain_performance)))\n",
    "        axes[1,1].set_yticklabels(domain_performance.index)\n",
    "        axes[1,1].set_xlabel('Average Quality Score')\n",
    "        axes[1,1].set_title('Performance by Domain (Top 10)')\n",
    "    \n",
    "    # 6. Execution Time by Complexity\n",
    "    if not successful_df.empty:\n",
    "        complexity_times = []\n",
    "        complexity_labels = []\n",
    "        for complexity in successful_df['expected_complexity'].unique():\n",
    "            times = successful_df[successful_df['expected_complexity'] == complexity]['execution_time']\n",
    "            complexity_times.append(times.values)\n",
    "            complexity_labels.append(complexity)\n",
    "        \n",
    "        axes[1,2].boxplot(complexity_times, labels=complexity_labels)\n",
    "        axes[1,2].set_ylabel('Execution Time (seconds)')\n",
    "        axes[1,2].set_xlabel('Complexity Level')\n",
    "        axes[1,2].set_title('Execution Time by Complexity')\n",
    "        axes[1,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"üìä Analysis and visualization functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and visualize current results\n",
    "if evaluation_results:\n",
    "    print(\"üìà Analyzing Current Results\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    metrics = analyze_evaluation_results(evaluation_results)\n",
    "    \n",
    "    print(f\"üìä Total Queries: {metrics['total_queries']}\")\n",
    "    print(f\"‚úÖ Successful: {metrics['successful']} ({metrics['success_rate']:.1%})\")\n",
    "    print(f\"‚ùå Failed: {metrics['failed']}\")\n",
    "    \n",
    "    if metrics['successful'] > 0:\n",
    "        print(f\"‚è±Ô∏è Avg Execution Time: {metrics['avg_execution_time']:.2f}s\")\n",
    "        print(f\"ü™ô Total Tokens: {metrics['total_tokens']:,}\")\n",
    "        print(f\"üìù Avg Response Length: {metrics['avg_response_length']:.0f} chars\")\n",
    "        print(f\"üìö Avg Citations: {metrics['avg_citations']:.1f}\")\n",
    "        print(f\"‚≠ê Avg Quality Score: {metrics['avg_quality_score']:.2f}\")\n",
    "        \n",
    "        if 'by_complexity' in metrics:\n",
    "            print(\"\\nüìã Performance by Complexity:\")\n",
    "            for complexity, comp_metrics in metrics['by_complexity'].items():\n",
    "                print(f\"  {complexity}: {comp_metrics['avg_time']:.2f}s, {comp_metrics['avg_tokens']:.0f} tokens\")\n",
    "    \n",
    "    # Show visualizations\n",
    "    print(\"\\nüìä Generating Visualizations...\")\n",
    "    visualize_results(evaluation_results)\n",
    "    \n",
    "else:\n",
    "    print(\"üìù No evaluation results yet. Use the controls above to run an evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export functions\n",
    "def export_results_to_csv(results: List[Dict], filename: str = None):\n",
    "    \"\"\"Export evaluation results to CSV file.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to export\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"üìÅ Results exported to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def export_results_to_json(results: List[Dict], filename: str = None):\n",
    "    \"\"\"Export evaluation results to JSON file.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to export\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üìÅ Results exported to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Export widgets\n",
    "export_csv_button = widgets.Button(\n",
    "    description='üíæ Export CSV',\n",
    "    button_style='info',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "export_json_button = widgets.Button(\n",
    "    description='üíæ Export JSON',\n",
    "    button_style='info', \n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "def on_export_csv_clicked(b):\n",
    "    if evaluation_results:\n",
    "        export_results_to_csv(evaluation_results)\n",
    "    else:\n",
    "        print(\"No results to export\")\n",
    "\n",
    "def on_export_json_clicked(b):\n",
    "    if evaluation_results:\n",
    "        export_results_to_json(evaluation_results)\n",
    "    else:\n",
    "        print(\"No results to export\")\n",
    "\n",
    "export_csv_button.on_click(on_export_csv_clicked)\n",
    "export_json_button.on_click(on_export_json_clicked)\n",
    "\n",
    "print(\"üíæ Export Controls\")\n",
    "display(widgets.HBox([export_csv_button, export_json_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Custom Query Testing\n",
    "\n",
    "Test the multi-agent system with your own custom queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom query testing interface\n",
    "custom_query_text = widgets.Textarea(\n",
    "    placeholder='Enter your custom query here...',\n",
    "    description='Custom Query:',\n",
    "    layout={'width': '100%', 'height': '100px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "test_button = widgets.Button(\n",
    "    description='üß™ Test Query',\n",
    "    button_style='warning',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "custom_output = widgets.Output()\n",
    "\n",
    "async def test_custom_query(query: str):\n",
    "    \"\"\"Test a custom query through the multi-agent system.\"\"\"\n",
    "    if not query.strip():\n",
    "        print(\"Please enter a query to test\")\n",
    "        return\n",
    "    \n",
    "    with custom_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"üß™ Testing query: {query}\")\n",
    "        print(\"‚è≥ Processing...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = await system.process_query(\n",
    "            query=query,\n",
    "            trace_id=f\"custom_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            session_id=\"custom_testing\"\n",
    "        )\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        with custom_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"üß™ Query: {query}\")\n",
    "            print(f\"‚úÖ Status: {result.get('status', 'unknown')}\")\n",
    "            print(f\"‚è±Ô∏è Execution Time: {execution_time:.2f}s\")\n",
    "            print(f\"ü™ô Total Tokens: {result.get('total_tokens', 0):,}\")\n",
    "            print(f\"üìö Citations: {len(result.get('citations', []))}\")\n",
    "            print(f\"ü§ñ Agents Used: {', '.join(result.get('agents_used', []))}\")\n",
    "            print(\"\\nüìÑ Response:\")\n",
    "            print(\"-\" * 50)\n",
    "            response = result.get('response', 'No response available')\n",
    "            if isinstance(response, dict) and 'extracted_content' in response:\n",
    "                print(response['extracted_content'][:500] + \"...\" if len(response['extracted_content']) > 500 else response['extracted_content'])\n",
    "            else:\n",
    "                print(str(response)[:500] + \"...\" if len(str(response)) > 500 else str(response))\n",
    "        \n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        with custom_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"‚ùå Error testing query: {str(e)}\")\n",
    "            print(f\"‚è±Ô∏è Time before error: {execution_time:.2f}s\")\n",
    "\n",
    "async def on_test_button_clicked(b):\n",
    "    query = custom_query_text.value\n",
    "    await test_custom_query(query)\n",
    "\n",
    "test_button.on_click(lambda b: asyncio.create_task(on_test_button_clicked(b)))\n",
    "\n",
    "print(\"üß™ Custom Query Testing\")\n",
    "display(widgets.VBox([\n",
    "    custom_query_text,\n",
    "    test_button,\n",
    "    custom_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup and Session Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup functions\n",
    "def clear_results():\n",
    "    \"\"\"Clear all evaluation results.\"\"\"\n",
    "    global evaluation_results\n",
    "    evaluation_results = []\n",
    "    print(\"üßπ Evaluation results cleared\")\n",
    "\n",
    "async def close_phoenix_session():\n",
    "    \"\"\"Close the current Phoenix session.\"\"\"\n",
    "    try:\n",
    "        final_metrics = await phoenix_integration.close_session()\n",
    "        print(f\"üî• Phoenix session closed: {final_metrics}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error closing Phoenix session: {str(e)}\")\n",
    "\n",
    "async def system_shutdown():\n",
    "    \"\"\"Gracefully shutdown the multi-agent system.\"\"\"\n",
    "    try:\n",
    "        await system.shutdown()\n",
    "        print(\"ü§ñ Multi-agent system shutdown complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error during system shutdown: {str(e)}\")\n",
    "\n",
    "# Cleanup buttons\n",
    "clear_button = widgets.Button(\n",
    "    description='üßπ Clear Results',\n",
    "    button_style='danger',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "close_session_button = widgets.Button(\n",
    "    description='üî• Close Phoenix',\n",
    "    button_style='warning',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "shutdown_button = widgets.Button(\n",
    "    description='üõë Shutdown System',\n",
    "    button_style='danger',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "clear_button.on_click(lambda b: clear_results())\n",
    "close_session_button.on_click(lambda b: asyncio.create_task(close_phoenix_session()))\n",
    "shutdown_button.on_click(lambda b: asyncio.create_task(system_shutdown()))\n",
    "\n",
    "print(\"üßπ Cleanup Controls\")\n",
    "display(widgets.HBox([clear_button, close_session_button, shutdown_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Usage Instructions\n",
    "\n",
    "### üöÄ Getting Started\n",
    "1. **Initialize**: Run the setup cells to initialize the multi-agent system\n",
    "2. **Configure**: Use the interactive controls to set evaluation parameters\n",
    "3. **Run**: Click \"üöÄ Run Evaluation\" to start the evaluation process\n",
    "4. **Analyze**: View results and visualizations in real-time\n",
    "5. **Export**: Save results to CSV or JSON for further analysis\n",
    "\n",
    "### üéÆ Interactive Controls\n",
    "- **Max Queries**: Limit the number of queries to evaluate\n",
    "- **Complexity Filter**: Select which model complexity levels to test\n",
    "- **Domain Filter**: Focus on specific domains or test all\n",
    "- **Progress Bar**: Real-time progress tracking during evaluation\n",
    "\n",
    "### üß™ Custom Testing\n",
    "- Use the \"Custom Query Testing\" section to test individual queries\n",
    "- Great for debugging or exploring system behavior\n",
    "\n",
    "### üìä Results Analysis\n",
    "- Comprehensive metrics including success rate, execution time, token usage\n",
    "- Visual analysis with multiple chart types\n",
    "- Performance breakdown by complexity level and domain\n",
    "\n",
    "### üî• Phoenix Integration\n",
    "- Real-time tracing and observability\n",
    "- Visit http://localhost:6006 to view Phoenix UI\n",
    "- Session management and cleanup functions\n",
    "\n",
    "### üí° Tips\n",
    "- Start with small batches (5-10 queries) to test functionality\n",
    "- Monitor Phoenix UI for detailed trace information\n",
    "- Export results regularly for backup\n",
    "- Use custom queries to test edge cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final status check\n",
    "print(\"üéâ Multi-Agent Evaluation Notebook Ready!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ü§ñ System Status: {'‚úÖ Active' if system.is_initialized else '‚ùå Inactive'}\")\n",
    "print(f\"üî• Phoenix Session: {session_id}\")\n",
    "print(f\"üìä Dataset Size: {len(EVALUATION_QUERIES)} queries\")\n",
    "print(f\"üìà Current Results: {len(evaluation_results)} evaluations\")\n",
    "print(\"\\nüöÄ Ready to run evaluations!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}