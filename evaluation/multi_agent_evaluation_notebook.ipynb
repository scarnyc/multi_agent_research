{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– Multi-Agent Research System Evaluation Notebook\n",
    "\n",
    "This notebook provides a comprehensive interface for running and evaluating the multi-agent research system with Phoenix observability.\n",
    "\n",
    "## ğŸ“‹ Features\n",
    "- Interactive evaluation controls\n",
    "- Real-time progress tracking\n",
    "- Phoenix tracing integration\n",
    "- Comprehensive result analysis\n",
    "- Quality metrics visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"ğŸ“ Project root: {project_root}\")\n",
    "print(f\"ğŸ Python path updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import multi-agent system components\n",
    "from agents.multi_agents import MultiAgentResearchSystem, initialize_system\n",
    "from agents.supervisor import SupervisorAgent\n",
    "from agents.search import SearchAgent\n",
    "from agents.citation import CitationAgent\n",
    "from config.settings import settings, ReasoningEffort, Verbosity\n",
    "from evaluation_dataset import EVALUATION_QUERIES, get_queries_by_complexity\n",
    "from evaluation.phoenix_integration import phoenix_integration, start_evaluation_session\n",
    "\n",
    "print(\"âœ… Multi-agent system imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging for notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set library loggers to WARNING to reduce noise\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "logging.getLogger('openai').setLevel(logging.WARNING)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "print(\"ğŸ“ Logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ System Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the multi-agent research system\n",
    "system = MultiAgentResearchSystem(\n",
    "    supervisor_reasoning=ReasoningEffort.MEDIUM,\n",
    "    supervisor_verbosity=Verbosity.MEDIUM,\n",
    "    search_reasoning=ReasoningEffort.LOW,\n",
    "    citation_reasoning=ReasoningEffort.LOW,\n",
    "    enable_phoenix_tracing=True\n",
    ")\n",
    "\n",
    "print(\"ğŸ¤– Multi-Agent Research System Initialized\")\n",
    "print(f\"ğŸ“Š Agents: {list(system.supervisor.sub_agents.keys())}\")\n",
    "print(f\"ğŸ” Phoenix Tracing: {'Enabled' if system.enable_phoenix_tracing else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Phoenix Integration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Phoenix evaluation session\n",
    "session_name = f\"notebook_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "session_id = await start_evaluation_session(session_name)\n",
    "\n",
    "print(f\"ğŸ”¥ Phoenix Session Started: {session_id}\")\n",
    "print(f\"ğŸ“ Access Phoenix UI at: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Evaluation Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation dataset summary\n",
    "def analyze_dataset():\n",
    "    \"\"\"Analyze and display evaluation dataset statistics.\"\"\"\n",
    "    total_queries = len(EVALUATION_QUERIES)\n",
    "    \n",
    "    # Group by complexity\n",
    "    complexity_counts = {}\n",
    "    query_types = {}\n",
    "    domains = {}\n",
    "    current_info_needed = 0\n",
    "    \n",
    "    for query in EVALUATION_QUERIES:\n",
    "        # Count by complexity\n",
    "        complexity = query.get('expected_model', 'unknown')\n",
    "        complexity_counts[complexity] = complexity_counts.get(complexity, 0) + 1\n",
    "        \n",
    "        # Count by type\n",
    "        query_type = query.get('type', 'unknown')\n",
    "        query_types[query_type] = query_types.get(query_type, 0) + 1\n",
    "        \n",
    "        # Count by domain\n",
    "        domain = query.get('domain', 'unknown')\n",
    "        domains[domain] = domains.get(domain, 0) + 1\n",
    "        \n",
    "        # Count current info requirements\n",
    "        if query.get('requires_current_info', False):\n",
    "            current_info_needed += 1\n",
    "    \n",
    "    return {\n",
    "        'total': total_queries,\n",
    "        'complexity': complexity_counts,\n",
    "        'types': query_types,\n",
    "        'domains': domains,\n",
    "        'current_info': current_info_needed\n",
    "    }\n",
    "\n",
    "dataset_stats = analyze_dataset()\n",
    "\n",
    "print(\"ğŸ“ˆ Evaluation Dataset Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“Š Total Queries: {dataset_stats['total']}\")\n",
    "print(f\"ğŸ” Require Current Info: {dataset_stats['current_info']}\")\n",
    "print()\n",
    "print(\"ğŸ“‹ By Complexity:\")\n",
    "for complexity, count in dataset_stats['complexity'].items():\n",
    "    print(f\"  {complexity}: {count} queries\")\n",
    "print()\n",
    "print(\"ğŸ“ By Type:\")\n",
    "for qtype, count in dataset_stats['types'].items():\n",
    "    print(f\"  {qtype}: {count} queries\")\n",
    "print()\n",
    "print(\"ğŸ·ï¸ By Domain (top 10):\")\n",
    "sorted_domains = sorted(dataset_stats['domains'].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for domain, count in sorted_domains:\n",
    "    print(f\"  {domain}: {count} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Evaluation Dataset Distribution', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Complexity distribution\n",
    "complexities = list(dataset_stats['complexity'].keys())\n",
    "complexity_counts = list(dataset_stats['complexity'].values())\n",
    "axes[0,0].pie(complexity_counts, labels=complexities, autopct='%1.1f%%', startangle=90)\n",
    "axes[0,0].set_title('Distribution by Complexity')\n",
    "\n",
    "# Query type distribution  \n",
    "types = list(dataset_stats['types'].keys())\n",
    "type_counts = list(dataset_stats['types'].values())\n",
    "axes[0,1].pie(type_counts, labels=types, autopct='%1.1f%%', startangle=90)\n",
    "axes[0,1].set_title('Distribution by Query Type')\n",
    "\n",
    "# Top domains\n",
    "top_domains = sorted_domains[:8]\n",
    "domain_names = [d[0] for d in top_domains]\n",
    "domain_counts = [d[1] for d in top_domains]\n",
    "axes[1,0].bar(range(len(domain_names)), domain_counts)\n",
    "axes[1,0].set_xticks(range(len(domain_names)))\n",
    "axes[1,0].set_xticklabels(domain_names, rotation=45, ha='right')\n",
    "axes[1,0].set_title('Top 8 Domains')\n",
    "axes[1,0].set_ylabel('Number of Queries')\n",
    "\n",
    "# Current info requirement\n",
    "current_info_data = ['Requires Current Info', 'Historical Info OK']\n",
    "current_info_counts = [dataset_stats['current_info'], dataset_stats['total'] - dataset_stats['current_info']]\n",
    "axes[1,1].pie(current_info_counts, labels=current_info_data, autopct='%1.1f%%', startangle=90)\n",
    "axes[1,1].set_title('Current Information Requirements')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ® Interactive Evaluation Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive widgets for evaluation control\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "# Evaluation control widgets\n",
    "max_queries_slider = widgets.IntSlider(\n",
    "    value=5,\n",
    "    min=1,\n",
    "    max=20,\n",
    "    step=1,\n",
    "    description='Max Queries:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "complexity_filter = widgets.SelectMultiple(\n",
    "    options=['gpt-5-nano', 'gpt-5-mini', 'gpt-5'],\n",
    "    value=['gpt-5-nano', 'gpt-5-mini', 'gpt-5'],\n",
    "    description='Complexity Levels:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "domain_filter = widgets.Dropdown(\n",
    "    options=['All'] + [d[0] for d in sorted_domains[:10]],\n",
    "    value='All',\n",
    "    description='Domain Filter:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='ğŸš€ Run Evaluation',\n",
    "    button_style='success',\n",
    "    layout={'width': '200px', 'height': '40px'}\n",
    ")\n",
    "\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progress:',\n",
    "    bar_style='info',\n",
    "    style={'bar_color': '#1f77b4', 'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "status_output = widgets.Output()\n",
    "\n",
    "print(\"ğŸ® Interactive Evaluation Controls\")\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([max_queries_slider]),\n",
    "    widgets.HBox([complexity_filter]),\n",
    "    widgets.HBox([domain_filter]),\n",
    "    widgets.HBox([run_button]),\n",
    "    widgets.HBox([progress_bar]),\n",
    "    status_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation results storage\n",
    "evaluation_results = []\n",
    "current_evaluation_session = None\n",
    "\n",
    "async def run_single_evaluation(query_data: Dict, session_id: str) -> Dict:\n",
    "    \"\"\"Run evaluation on a single query and collect metrics.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Process query through multi-agent system\n",
    "        result = await system.process_query(\n",
    "            query=query_data['query'],\n",
    "            trace_id=f\"{session_id}_{query_data.get('id', 'unknown')}\",\n",
    "            session_id=session_id\n",
    "        )\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # Extract key metrics\n",
    "        response_length = len(result.get('response', ''))\n",
    "        citations_count = len(result.get('citations', []))\n",
    "        total_tokens = result.get('total_tokens', 0)\n",
    "        \n",
    "        # Simple quality metrics (would be enhanced with proper evaluation models)\n",
    "        quality_score = min(1.0, response_length / 500)  # Basic length-based score\n",
    "        citation_completeness = min(1.0, citations_count / query_data.get('expected_sources', 5))\n",
    "        \n",
    "        return {\n",
    "            'query_id': query_data.get('id', 'unknown'),\n",
    "            'query': query_data['query'],\n",
    "            'expected_complexity': query_data.get('expected_model', 'unknown'),\n",
    "            'domain': query_data.get('domain', 'unknown'),\n",
    "            'type': query_data.get('type', 'unknown'),\n",
    "            'status': result.get('status', 'unknown'),\n",
    "            'response_length': response_length,\n",
    "            'citations_count': citations_count,\n",
    "            'total_tokens': total_tokens,\n",
    "            'execution_time': execution_time,\n",
    "            'quality_score': quality_score,\n",
    "            'citation_completeness': citation_completeness,\n",
    "            'agents_used': result.get('agents_used', []),\n",
    "            'model_used': result.get('model_used', 'unknown'),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'success': result.get('status') == 'success'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        logger.error(f\"Evaluation failed for query {query_data.get('id', 'unknown')}: {str(e)}\")\n",
    "        \n",
    "        return {\n",
    "            'query_id': query_data.get('id', 'unknown'),\n",
    "            'query': query_data['query'],\n",
    "            'expected_complexity': query_data.get('expected_model', 'unknown'),\n",
    "            'domain': query_data.get('domain', 'unknown'),\n",
    "            'type': query_data.get('type', 'unknown'),\n",
    "            'status': 'error',\n",
    "            'response_length': 0,\n",
    "            'citations_count': 0,\n",
    "            'total_tokens': 0,\n",
    "            'execution_time': execution_time,\n",
    "            'quality_score': 0.0,\n",
    "            'citation_completeness': 0.0,\n",
    "            'agents_used': [],\n",
    "            'model_used': 'error',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "async def run_evaluation_batch(queries: List[Dict], session_id: str) -> List[Dict]:\n",
    "    \"\"\"Run evaluation on a batch of queries with progress tracking.\"\"\"\n",
    "    results = []\n",
    "    total = len(queries)\n",
    "    \n",
    "    with status_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"ğŸš€ Starting evaluation of {total} queries...\")\n",
    "    \n",
    "    for i, query_data in enumerate(queries):\n",
    "        # Update progress\n",
    "        progress_bar.value = int((i / total) * 100)\n",
    "        \n",
    "        with status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"ğŸ“ Processing query {i+1}/{total}: {query_data['query'][:50]}...\")\n",
    "        \n",
    "        # Run single evaluation\n",
    "        result = await run_single_evaluation(query_data, session_id)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Brief pause to prevent overwhelming the API\n",
    "        await asyncio.sleep(0.5)\n",
    "    \n",
    "    progress_bar.value = 100\n",
    "    with status_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"âœ… Evaluation complete! Processed {len(results)} queries\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"ğŸ“Š Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Button click handler\n",
    "async def on_run_button_clicked(b):\n",
    "    \"\"\"Handle run evaluation button click.\"\"\"\n",
    "    global evaluation_results, current_evaluation_session\n",
    "    \n",
    "    # Get filter settings\n",
    "    max_queries = max_queries_slider.value\n",
    "    selected_complexities = list(complexity_filter.value)\n",
    "    selected_domain = domain_filter.value\n",
    "    \n",
    "    # Filter queries based on settings\n",
    "    filtered_queries = []\n",
    "    for query in EVALUATION_QUERIES:\n",
    "        # Filter by complexity\n",
    "        if query.get('expected_model', 'unknown') not in selected_complexities:\n",
    "            continue\n",
    "            \n",
    "        # Filter by domain\n",
    "        if selected_domain != 'All' and query.get('domain', 'unknown') != selected_domain:\n",
    "            continue\n",
    "            \n",
    "        filtered_queries.append(query)\n",
    "        \n",
    "        # Limit to max queries\n",
    "        if len(filtered_queries) >= max_queries:\n",
    "            break\n",
    "    \n",
    "    if not filtered_queries:\n",
    "        with status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"âš ï¸ No queries match the selected filters!\")\n",
    "        return\n",
    "    \n",
    "    # Start new evaluation session\n",
    "    eval_session_id = f\"interactive_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    current_evaluation_session = eval_session_id\n",
    "    \n",
    "    # Run evaluation\n",
    "    try:\n",
    "        run_button.disabled = True\n",
    "        run_button.description = 'â³ Running...'\n",
    "        \n",
    "        results = await run_evaluation_batch(filtered_queries, eval_session_id)\n",
    "        evaluation_results.extend(results)\n",
    "        \n",
    "        # Show summary\n",
    "        successful = len([r for r in results if r['success']])\n",
    "        failed = len(results) - successful\n",
    "        avg_time = sum(r['execution_time'] for r in results) / len(results)\n",
    "        total_tokens = sum(r['total_tokens'] for r in results)\n",
    "        \n",
    "        with status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"ğŸ‰ Evaluation Complete!\")\n",
    "            print(f\"âœ… Successful: {successful}\")\n",
    "            print(f\"âŒ Failed: {failed}\")\n",
    "            print(f\"â±ï¸ Avg Time: {avg_time:.2f}s\")\n",
    "            print(f\"ğŸª™ Total Tokens: {total_tokens:,}\")\n",
    "            print(f\"ğŸ“Š Results stored in evaluation_results\")\n",
    "        \n",
    "    finally:\n",
    "        run_button.disabled = False\n",
    "        run_button.description = 'ğŸš€ Run Evaluation'\n",
    "\n",
    "# Connect button to handler\n",
    "run_button.on_click(lambda b: asyncio.create_task(on_run_button_clicked(b)))\n",
    "\n",
    "print(\"ğŸ® Button handler connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Quick Test Evaluation\n",
    "\n",
    "Let's run a quick test with a few sample queries to verify everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test evaluation\n",
    "test_queries = [\n",
    "    {\n",
    "        'id': 'test_1',\n",
    "        'query': 'What is artificial intelligence?',\n",
    "        'expected_model': 'gpt-5-nano',\n",
    "        'domain': 'technology',\n",
    "        'type': 'qa',\n",
    "        'expected_sources': 3\n",
    "    },\n",
    "    {\n",
    "        'id': 'test_2', \n",
    "        'query': 'Explain the latest developments in quantum computing and their implications for cryptography',\n",
    "        'expected_model': 'gpt-5',\n",
    "        'domain': 'technology',\n",
    "        'type': 'research',\n",
    "        'expected_sources': 5\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Running quick test evaluation...\")\n",
    "test_session_id = f\"test_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "test_results = await run_evaluation_batch(test_queries, test_session_id)\n",
    "\n",
    "# Display test results\n",
    "print(\"\\nğŸ“Š Test Results:\")\n",
    "for result in test_results:\n",
    "    status_emoji = \"âœ…\" if result['success'] else \"âŒ\"\n",
    "    print(f\"{status_emoji} {result['query_id']}: {result['execution_time']:.2f}s, {result['total_tokens']} tokens, {result['citations_count']} citations\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Test completed! {len([r for r in test_results if r['success']])}/{len(test_results)} successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results analysis functions\n",
    "def analyze_evaluation_results(results: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze evaluation results and compute comprehensive metrics.\"\"\"\n",
    "    if not results:\n",
    "        return {\"error\": \"No results to analyze\"}\n",
    "    \n",
    "    successful = [r for r in results if r['success']]\n",
    "    failed = [r for r in results if not r['success']]\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics = {\n",
    "        'total_queries': len(results),\n",
    "        'successful': len(successful),\n",
    "        'failed': len(failed),\n",
    "        'success_rate': len(successful) / len(results) if results else 0,\n",
    "    }\n",
    "    \n",
    "    if successful:\n",
    "        # Performance metrics\n",
    "        execution_times = [r['execution_time'] for r in successful]\n",
    "        token_counts = [r['total_tokens'] for r in successful]\n",
    "        response_lengths = [r['response_length'] for r in successful]\n",
    "        citation_counts = [r['citations_count'] for r in successful]\n",
    "        quality_scores = [r['quality_score'] for r in successful]\n",
    "        \n",
    "        metrics.update({\n",
    "            'avg_execution_time': sum(execution_times) / len(execution_times),\n",
    "            'median_execution_time': sorted(execution_times)[len(execution_times)//2],\n",
    "            'max_execution_time': max(execution_times),\n",
    "            'min_execution_time': min(execution_times),\n",
    "            'total_tokens': sum(token_counts),\n",
    "            'avg_tokens': sum(token_counts) / len(token_counts),\n",
    "            'avg_response_length': sum(response_lengths) / len(response_lengths),\n",
    "            'avg_citations': sum(citation_counts) / len(citation_counts),\n",
    "            'avg_quality_score': sum(quality_scores) / len(quality_scores),\n",
    "        })\n",
    "        \n",
    "        # Performance by complexity\n",
    "        by_complexity = {}\n",
    "        for result in successful:\n",
    "            complexity = result['expected_complexity']\n",
    "            if complexity not in by_complexity:\n",
    "                by_complexity[complexity] = []\n",
    "            by_complexity[complexity].append(result)\n",
    "        \n",
    "        complexity_metrics = {}\n",
    "        for complexity, comp_results in by_complexity.items():\n",
    "            complexity_metrics[complexity] = {\n",
    "                'count': len(comp_results),\n",
    "                'avg_time': sum(r['execution_time'] for r in comp_results) / len(comp_results),\n",
    "                'avg_tokens': sum(r['total_tokens'] for r in comp_results) / len(comp_results),\n",
    "                'avg_quality': sum(r['quality_score'] for r in comp_results) / len(comp_results),\n",
    "            }\n",
    "        \n",
    "        metrics['by_complexity'] = complexity_metrics\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def visualize_results(results: List[Dict]):\n",
    "    \"\"\"Create comprehensive visualizations of evaluation results.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Multi-Agent System Evaluation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Success Rate\n",
    "    success_counts = df['success'].value_counts()\n",
    "    axes[0,0].pie(success_counts.values, labels=['Success', 'Failed'], autopct='%1.1f%%', \n",
    "                  colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
    "    axes[0,0].set_title('Success Rate')\n",
    "    \n",
    "    # 2. Execution Time Distribution\n",
    "    successful_df = df[df['success'] == True]\n",
    "    if not successful_df.empty:\n",
    "        axes[0,1].hist(successful_df['execution_time'], bins=15, alpha=0.7, color='#3498db')\n",
    "        axes[0,1].set_xlabel('Execution Time (seconds)')\n",
    "        axes[0,1].set_ylabel('Frequency')\n",
    "        axes[0,1].set_title('Execution Time Distribution')\n",
    "        axes[0,1].axvline(successful_df['execution_time'].mean(), color='red', linestyle='--', \n",
    "                         label=f'Mean: {successful_df[\"execution_time\"].mean():.2f}s')\n",
    "        axes[0,1].legend()\n",
    "    \n",
    "    # 3. Token Usage by Complexity\n",
    "    if not successful_df.empty:\n",
    "        complexity_tokens = successful_df.groupby('expected_complexity')['total_tokens'].mean()\n",
    "        axes[0,2].bar(complexity_tokens.index, complexity_tokens.values, color='#9b59b6')\n",
    "        axes[0,2].set_xlabel('Complexity Level')\n",
    "        axes[0,2].set_ylabel('Average Tokens')\n",
    "        axes[0,2].set_title('Token Usage by Complexity')\n",
    "        axes[0,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Response Length vs Citations\n",
    "    if not successful_df.empty:\n",
    "        scatter = axes[1,0].scatter(successful_df['response_length'], successful_df['citations_count'], \n",
    "                                   c=successful_df['execution_time'], cmap='viridis', alpha=0.6)\n",
    "        axes[1,0].set_xlabel('Response Length (characters)')\n",
    "        axes[1,0].set_ylabel('Number of Citations')\n",
    "        axes[1,0].set_title('Response Length vs Citations')\n",
    "        plt.colorbar(scatter, ax=axes[1,0], label='Execution Time (s)')\n",
    "    \n",
    "    # 5. Performance by Domain\n",
    "    if not successful_df.empty:\n",
    "        domain_performance = successful_df.groupby('domain')['quality_score'].mean().sort_values(ascending=False)\n",
    "        if len(domain_performance) > 10:\n",
    "            domain_performance = domain_performance.head(10)\n",
    "        axes[1,1].barh(range(len(domain_performance)), domain_performance.values, color='#e67e22')\n",
    "        axes[1,1].set_yticks(range(len(domain_performance)))\n",
    "        axes[1,1].set_yticklabels(domain_performance.index)\n",
    "        axes[1,1].set_xlabel('Average Quality Score')\n",
    "        axes[1,1].set_title('Performance by Domain (Top 10)')\n",
    "    \n",
    "    # 6. Execution Time by Complexity\n",
    "    if not successful_df.empty:\n",
    "        complexity_times = []\n",
    "        complexity_labels = []\n",
    "        for complexity in successful_df['expected_complexity'].unique():\n",
    "            times = successful_df[successful_df['expected_complexity'] == complexity]['execution_time']\n",
    "            complexity_times.append(times.values)\n",
    "            complexity_labels.append(complexity)\n",
    "        \n",
    "        axes[1,2].boxplot(complexity_times, labels=complexity_labels)\n",
    "        axes[1,2].set_ylabel('Execution Time (seconds)')\n",
    "        axes[1,2].set_xlabel('Complexity Level')\n",
    "        axes[1,2].set_title('Execution Time by Complexity')\n",
    "        axes[1,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Analysis and visualization functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and visualize current results\n",
    "if evaluation_results:\n",
    "    print(\"ğŸ“ˆ Analyzing Current Results\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    metrics = analyze_evaluation_results(evaluation_results)\n",
    "    \n",
    "    print(f\"ğŸ“Š Total Queries: {metrics['total_queries']}\")\n",
    "    print(f\"âœ… Successful: {metrics['successful']} ({metrics['success_rate']:.1%})\")\n",
    "    print(f\"âŒ Failed: {metrics['failed']}\")\n",
    "    \n",
    "    if metrics['successful'] > 0:\n",
    "        print(f\"â±ï¸ Avg Execution Time: {metrics['avg_execution_time']:.2f}s\")\n",
    "        print(f\"ğŸª™ Total Tokens: {metrics['total_tokens']:,}\")\n",
    "        print(f\"ğŸ“ Avg Response Length: {metrics['avg_response_length']:.0f} chars\")\n",
    "        print(f\"ğŸ“š Avg Citations: {metrics['avg_citations']:.1f}\")\n",
    "        print(f\"â­ Avg Quality Score: {metrics['avg_quality_score']:.2f}\")\n",
    "        \n",
    "        if 'by_complexity' in metrics:\n",
    "            print(\"\\nğŸ“‹ Performance by Complexity:\")\n",
    "            for complexity, comp_metrics in metrics['by_complexity'].items():\n",
    "                print(f\"  {complexity}: {comp_metrics['avg_time']:.2f}s, {comp_metrics['avg_tokens']:.0f} tokens\")\n",
    "    \n",
    "    # Show visualizations\n",
    "    print(\"\\nğŸ“Š Generating Visualizations...\")\n",
    "    visualize_results(evaluation_results)\n",
    "    \n",
    "else:\n",
    "    print(\"ğŸ“ No evaluation results yet. Use the controls above to run an evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export functions\n",
    "def export_results_to_csv(results: List[Dict], filename: str = None):\n",
    "    \"\"\"Export evaluation results to CSV file.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to export\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"ğŸ“ Results exported to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def export_results_to_json(results: List[Dict], filename: str = None):\n",
    "    \"\"\"Export evaluation results to JSON file.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to export\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"ğŸ“ Results exported to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Export widgets\n",
    "export_csv_button = widgets.Button(\n",
    "    description='ğŸ’¾ Export CSV',\n",
    "    button_style='info',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "export_json_button = widgets.Button(\n",
    "    description='ğŸ’¾ Export JSON',\n",
    "    button_style='info', \n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "def on_export_csv_clicked(b):\n",
    "    if evaluation_results:\n",
    "        export_results_to_csv(evaluation_results)\n",
    "    else:\n",
    "        print(\"No results to export\")\n",
    "\n",
    "def on_export_json_clicked(b):\n",
    "    if evaluation_results:\n",
    "        export_results_to_json(evaluation_results)\n",
    "    else:\n",
    "        print(\"No results to export\")\n",
    "\n",
    "export_csv_button.on_click(on_export_csv_clicked)\n",
    "export_json_button.on_click(on_export_json_clicked)\n",
    "\n",
    "print(\"ğŸ’¾ Export Controls\")\n",
    "display(widgets.HBox([export_csv_button, export_json_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Custom Query Testing\n",
    "\n",
    "Test the multi-agent system with your own custom queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom query testing interface\n",
    "custom_query_text = widgets.Textarea(\n",
    "    placeholder='Enter your custom query here...',\n",
    "    description='Custom Query:',\n",
    "    layout={'width': '100%', 'height': '100px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "test_button = widgets.Button(\n",
    "    description='ğŸ§ª Test Query',\n",
    "    button_style='warning',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "custom_output = widgets.Output()\n",
    "\n",
    "async def test_custom_query(query: str):\n",
    "    \"\"\"Test a custom query through the multi-agent system.\"\"\"\n",
    "    if not query.strip():\n",
    "        print(\"Please enter a query to test\")\n",
    "        return\n",
    "    \n",
    "    with custom_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"ğŸ§ª Testing query: {query}\")\n",
    "        print(\"â³ Processing...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = await system.process_query(\n",
    "            query=query,\n",
    "            trace_id=f\"custom_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            session_id=\"custom_testing\"\n",
    "        )\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        with custom_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"ğŸ§ª Query: {query}\")\n",
    "            print(f\"âœ… Status: {result.get('status', 'unknown')}\")\n",
    "            print(f\"â±ï¸ Execution Time: {execution_time:.2f}s\")\n",
    "            print(f\"ğŸª™ Total Tokens: {result.get('total_tokens', 0):,}\")\n",
    "            print(f\"ğŸ“š Citations: {len(result.get('citations', []))}\")\n",
    "            print(f\"ğŸ¤– Agents Used: {', '.join(result.get('agents_used', []))}\")\n",
    "            print(\"\\nğŸ“„ Response:\")\n",
    "            print(\"-\" * 50)\n",
    "            response = result.get('response', 'No response available')\n",
    "            if isinstance(response, dict) and 'extracted_content' in response:\n",
    "                print(response['extracted_content'][:500] + \"...\" if len(response['extracted_content']) > 500 else response['extracted_content'])\n",
    "            else:\n",
    "                print(str(response)[:500] + \"...\" if len(str(response)) > 500 else str(response))\n",
    "        \n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        with custom_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"âŒ Error testing query: {str(e)}\")\n",
    "            print(f\"â±ï¸ Time before error: {execution_time:.2f}s\")\n",
    "\n",
    "async def on_test_button_clicked(b):\n",
    "    query = custom_query_text.value\n",
    "    await test_custom_query(query)\n",
    "\n",
    "test_button.on_click(lambda b: asyncio.create_task(on_test_button_clicked(b)))\n",
    "\n",
    "print(\"ğŸ§ª Custom Query Testing\")\n",
    "display(widgets.VBox([\n",
    "    custom_query_text,\n",
    "    test_button,\n",
    "    custom_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Cleanup and Session Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup functions\n",
    "def clear_results():\n",
    "    \"\"\"Clear all evaluation results.\"\"\"\n",
    "    global evaluation_results\n",
    "    evaluation_results = []\n",
    "    print(\"ğŸ§¹ Evaluation results cleared\")\n",
    "\n",
    "async def close_phoenix_session():\n",
    "    \"\"\"Close the current Phoenix session.\"\"\"\n",
    "    try:\n",
    "        final_metrics = await phoenix_integration.close_session()\n",
    "        print(f\"ğŸ”¥ Phoenix session closed: {final_metrics}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error closing Phoenix session: {str(e)}\")\n",
    "\n",
    "async def system_shutdown():\n",
    "    \"\"\"Gracefully shutdown the multi-agent system.\"\"\"\n",
    "    try:\n",
    "        await system.shutdown()\n",
    "        print(\"ğŸ¤– Multi-agent system shutdown complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error during system shutdown: {str(e)}\")\n",
    "\n",
    "# Cleanup buttons\n",
    "clear_button = widgets.Button(\n",
    "    description='ğŸ§¹ Clear Results',\n",
    "    button_style='danger',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "close_session_button = widgets.Button(\n",
    "    description='ğŸ”¥ Close Phoenix',\n",
    "    button_style='warning',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "shutdown_button = widgets.Button(\n",
    "    description='ğŸ›‘ Shutdown System',\n",
    "    button_style='danger',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "clear_button.on_click(lambda b: clear_results())\n",
    "close_session_button.on_click(lambda b: asyncio.create_task(close_phoenix_session()))\n",
    "shutdown_button.on_click(lambda b: asyncio.create_task(system_shutdown()))\n",
    "\n",
    "print(\"ğŸ§¹ Cleanup Controls\")\n",
    "display(widgets.HBox([clear_button, close_session_button, shutdown_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Usage Instructions\n",
    "\n",
    "### ğŸš€ Getting Started\n",
    "1. **Initialize**: Run the setup cells to initialize the multi-agent system\n",
    "2. **Configure**: Use the interactive controls to set evaluation parameters\n",
    "3. **Run**: Click \"ğŸš€ Run Evaluation\" to start the evaluation process\n",
    "4. **Analyze**: View results and visualizations in real-time\n",
    "5. **Export**: Save results to CSV or JSON for further analysis\n",
    "\n",
    "### ğŸ® Interactive Controls\n",
    "- **Max Queries**: Limit the number of queries to evaluate\n",
    "- **Complexity Filter**: Select which model complexity levels to test\n",
    "- **Domain Filter**: Focus on specific domains or test all\n",
    "- **Progress Bar**: Real-time progress tracking during evaluation\n",
    "\n",
    "### ğŸ§ª Custom Testing\n",
    "- Use the \"Custom Query Testing\" section to test individual queries\n",
    "- Great for debugging or exploring system behavior\n",
    "\n",
    "### ğŸ“Š Results Analysis\n",
    "- Comprehensive metrics including success rate, execution time, token usage\n",
    "- Visual analysis with multiple chart types\n",
    "- Performance breakdown by complexity level and domain\n",
    "\n",
    "### ğŸ”¥ Phoenix Integration\n",
    "- Real-time tracing and observability\n",
    "- Visit http://localhost:6006 to view Phoenix UI\n",
    "- Session management and cleanup functions\n",
    "\n",
    "### ğŸ’¡ Tips\n",
    "- Start with small batches (5-10 queries) to test functionality\n",
    "- Monitor Phoenix UI for detailed trace information\n",
    "- Export results regularly for backup\n",
    "- Use custom queries to test edge cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final status check\n",
    "print(\"ğŸ‰ Multi-Agent Evaluation Notebook Ready!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ¤– System Status: {'âœ… Active' if system.is_initialized else 'âŒ Inactive'}\")\n",
    "print(f\"ğŸ”¥ Phoenix Session: {session_id}\")\n",
    "print(f\"ğŸ“Š Dataset Size: {len(EVALUATION_QUERIES)} queries\")\n",
    "print(f\"ğŸ“ˆ Current Results: {len(evaluation_results)} evaluations\")\n",
    "print(\"\\nğŸš€ Ready to run evaluations!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}