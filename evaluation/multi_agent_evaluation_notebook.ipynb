{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Multi-Agent Research System Evaluation Notebook\n",
    "\n",
    "This notebook provides a comprehensive interface for running and evaluating the **user-centric multi-agent research system** with Phoenix observability.\n",
    "\n",
    "## üìã Features\n",
    "- **User-centric task routing** evaluation (DIRECT_ANSWER, SEARCH_NEEDED, RESEARCH_REPORT)\n",
    "- **Autonomous agent model selection** testing\n",
    "- Real-time progress tracking with Phoenix tracing\n",
    "- Comprehensive result analysis and quality metrics\n",
    "- Interactive controls and custom query testing\n",
    "- Export capabilities (CSV/JSON)\n",
    "\n",
    "## üéØ Recent Updates (September 2025)\n",
    "- **NEW**: User-centric architecture (replaces complexity-based routing)\n",
    "- **ENHANCED**: Direct Phoenix SDK integration (no more MCP issues)\n",
    "- **IMPROVED**: 100% test coverage and reliability\n",
    "- **ADDED**: TaskType-based evaluation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"üìÅ Project root: {project_root}\")\n",
    "print(f\"üêç Python path updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import multi-agent system components\n",
    "from agents.multi_agents import MultiAgentResearchSystem, initialize_system\n",
    "from agents.supervisor import SupervisorAgent\n",
    "from agents.search import SearchAgent\n",
    "from agents.citation import CitationAgent\n",
    "from config.settings import settings, ReasoningEffort, Verbosity, TaskType, ModelType\n",
    "from evaluation.evaluation_dataset import EVALUATION_QUERIES, EvalQuery\n",
    "from evaluation.phoenix_integration import phoenix_integration\n",
    "\n",
    "print(\"‚úÖ Multi-agent system imports successful\")\n",
    "print(f\"üìä Available Task Types: {[t.value for t in TaskType]}\")\n",
    "print(f\"ü§ñ Available Model Types: {[m.value for m in ModelType]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging for notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set library loggers to WARNING to reduce noise\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "logging.getLogger('openai').setLevel(logging.WARNING)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "print(\"üìù Logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ System Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the multi-agent research system\n",
    "system = MultiAgentResearchSystem(\n",
    "    supervisor_reasoning=ReasoningEffort.MEDIUM,\n",
    "    supervisor_verbosity=Verbosity.MEDIUM,\n",
    "    enable_phoenix_tracing=True\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Multi-Agent Research System Initialized\")\n",
    "print(f\"üìä Agents: {list(system.supervisor.sub_agents.keys())}\")\n",
    "print(f\"üéØ Task Types Supported: {[t.value for t in TaskType]}\")\n",
    "print(f\"üîç Phoenix Tracing: {'Enabled' if system.enable_phoenix_tracing else 'Disabled'}\")\n",
    "print(f\"üß† User-Centric Routing: Active (analyzes user intent, not complexity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Phoenix Integration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Phoenix evaluation session\n",
    "session_name = f\"notebook_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Use direct Phoenix SDK integration\n",
    "try:\n",
    "    session_id = phoenix_integration.start_evaluation_session(session_name)\n",
    "    print(f\"üî• Phoenix Session Started: {session_id}\")\n",
    "    print(f\"üìç Access Phoenix UI at: http://localhost:6006\")\n",
    "    print(f\"üí° Using Direct SDK Integration (no MCP issues)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Phoenix integration unavailable: {e}\")\n",
    "    print(\"üìù Evaluations will run without Phoenix tracing\")\n",
    "    session_id = f\"fallback_{session_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Evaluation Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation dataset summary\n",
    "def analyze_dataset():\n",
    "    \"\"\"Analyze and display evaluation dataset statistics.\"\"\"\n",
    "    total_queries = len(EVALUATION_QUERIES)\n",
    "    \n",
    "    # Group by task type (user-centric approach)\n",
    "    task_type_counts = {}\n",
    "    domains = {}\n",
    "    current_info_needed = 0\n",
    "    query_types = {}\n",
    "    \n",
    "    for query in EVALUATION_QUERIES:\n",
    "        # Count by task type (replaces complexity)\n",
    "        task_type = query.task_type.value if hasattr(query.task_type, 'value') else str(query.task_type)\n",
    "        task_type_counts[task_type] = task_type_counts.get(task_type, 0) + 1\n",
    "        \n",
    "        # Count by query type\n",
    "        query_type = query.query_type\n",
    "        query_types[query_type] = query_types.get(query_type, 0) + 1\n",
    "        \n",
    "        # Count by domain\n",
    "        domain = query.domain\n",
    "        domains[domain] = domains.get(domain, 0) + 1\n",
    "        \n",
    "        # Count current info requirements\n",
    "        if query.requires_current_info:\n",
    "            current_info_needed += 1\n",
    "    \n",
    "    return {\n",
    "        'total': total_queries,\n",
    "        'task_types': task_type_counts,  # Changed from 'complexity'\n",
    "        'query_types': query_types,\n",
    "        'domains': domains,\n",
    "        'current_info': current_info_needed\n",
    "    }\n",
    "\n",
    "dataset_stats = analyze_dataset()\n",
    "\n",
    "print(\"üìà Evaluation Dataset Analysis (User-Centric Architecture)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä Total Queries: {dataset_stats['total']}\")\n",
    "print(f\"üîç Require Current Info: {dataset_stats['current_info']}\")\n",
    "print()\n",
    "print(\"üéØ By Task Type (User Intent):\")\n",
    "for task_type, count in dataset_stats['task_types'].items():\n",
    "    print(f\"  {task_type}: {count} queries\")\n",
    "print()\n",
    "print(\"üìù By Query Type:\")\n",
    "for qtype, count in dataset_stats['query_types'].items():\n",
    "    print(f\"  {qtype}: {count} queries\")\n",
    "print()\n",
    "print(\"üè∑Ô∏è By Domain (top 10):\")\n",
    "sorted_domains = sorted(dataset_stats['domains'].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for domain, count in sorted_domains:\n",
    "    print(f\"  {domain}: {count} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Evaluation Dataset Distribution (User-Centric Architecture)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Task type distribution (replaces complexity)\n",
    "task_types = list(dataset_stats['task_types'].keys())\n",
    "task_type_counts = list(dataset_stats['task_types'].values())\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']  # Blue, Red, Green for the three task types\n",
    "axes[0,0].pie(task_type_counts, labels=task_types, autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "axes[0,0].set_title('Distribution by Task Type\\n(User Intent Analysis)')\n",
    "\n",
    "# Query type distribution  \n",
    "qtypes = list(dataset_stats['query_types'].keys())\n",
    "qtype_counts = list(dataset_stats['query_types'].values())\n",
    "axes[0,1].pie(qtype_counts, labels=qtypes, autopct='%1.1f%%', startangle=90)\n",
    "axes[0,1].set_title('Distribution by Query Type')\n",
    "\n",
    "# Top domains\n",
    "top_domains = sorted_domains[:8]\n",
    "domain_names = [d[0] for d in top_domains]\n",
    "domain_counts = [d[1] for d in top_domains]\n",
    "axes[1,0].bar(range(len(domain_names)), domain_counts, color='#9b59b6')\n",
    "axes[1,0].set_xticks(range(len(domain_names)))\n",
    "axes[1,0].set_xticklabels(domain_names, rotation=45, ha='right')\n",
    "axes[1,0].set_title('Top 8 Domains')\n",
    "axes[1,0].set_ylabel('Number of Queries')\n",
    "\n",
    "# Current info requirement\n",
    "current_info_data = ['Requires Current Info', 'Training Data OK']\n",
    "current_info_counts = [dataset_stats['current_info'], dataset_stats['total'] - dataset_stats['current_info']]\n",
    "axes[1,1].pie(current_info_counts, labels=current_info_data, autopct='%1.1f%%', startangle=90)\n",
    "axes[1,1].set_title('Information Recency Requirements')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display task type legend\n",
    "print(\"üéØ Task Type Legend:\")\n",
    "print(\"  üîµ DIRECT_ANSWER: Factual questions from training data\")  \n",
    "print(\"  üî¥ SEARCH_NEEDED: Questions requiring current information\")\n",
    "print(\"  üü¢ RESEARCH_REPORT: Deep analysis requiring sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Interactive Evaluation Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive widgets for evaluation control\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "# Evaluation control widgets\n",
    "max_queries_slider = widgets.IntSlider(\n",
    "    value=5,\n",
    "    min=1,\n",
    "    max=20,\n",
    "    step=1,\n",
    "    description='Max Queries:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Task type filter (replaces complexity filter)\n",
    "task_type_filter = widgets.SelectMultiple(\n",
    "    options=['direct_answer', 'search_needed', 'research_report'],\n",
    "    value=['direct_answer', 'search_needed', 'research_report'],\n",
    "    description='Task Types:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px', 'height': '80px'}\n",
    ")\n",
    "\n",
    "domain_filter = widgets.Dropdown(\n",
    "    options=['All'] + [d[0] for d in sorted_domains[:10]],\n",
    "    value='All',\n",
    "    description='Domain Filter:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='üöÄ Run Evaluation',\n",
    "    button_style='success',\n",
    "    layout={'width': '200px', 'height': '40px'}\n",
    ")\n",
    "\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progress:',\n",
    "    bar_style='info',\n",
    "    style={'bar_color': '#1f77b4', 'description_width': 'initial'},\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "status_output = widgets.Output()\n",
    "\n",
    "print(\"üéÆ Interactive Evaluation Controls (User-Centric Architecture)\")\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([max_queries_slider]),\n",
    "    widgets.HBox([task_type_filter]),\n",
    "    widgets.HBox([domain_filter]),\n",
    "    widgets.HBox([run_button]),\n",
    "    widgets.HBox([progress_bar]),\n",
    "    status_output\n",
    "]))\n",
    "\n",
    "# Display task type information\n",
    "print(\"\\nüéØ Task Type Filters:\")\n",
    "print(\"  üîµ direct_answer: Questions answerable from training data\")\n",
    "print(\"  üî¥ search_needed: Questions requiring current information\")\n",
    "print(\"  üü¢ research_report: Complex analysis requiring sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation results storage\n",
    "evaluation_results = []\n",
    "current_evaluation_session = None\n",
    "\n",
    "async def run_single_evaluation(query_data: EvalQuery, session_id: str) -> Dict:\n",
    "    \"\"\"Run evaluation on a single query and collect metrics.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Process query through multi-agent system\n",
    "        result = await system.process_query(\n",
    "            query=query_data.query,\n",
    "            trace_id=f\"{session_id}_{query_data.id}\",\n",
    "            session_id=session_id\n",
    "        )\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # Extract key metrics\n",
    "        response_length = len(result.get('response', ''))\n",
    "        citations_count = len(result.get('citations', []))\n",
    "        total_tokens = result.get('total_tokens', 0)\n",
    "        \n",
    "        # Enhanced quality metrics for user-centric architecture\n",
    "        quality_score = min(1.0, response_length / 500)  # Basic length-based score\n",
    "        citation_completeness = min(1.0, citations_count / query_data.expected_sources)\n",
    "        \n",
    "        # Check if task type routing worked correctly\n",
    "        expected_task_type = query_data.task_type.value if hasattr(query_data.task_type, 'value') else str(query_data.task_type)\n",
    "        actual_task_type = result.get('task_type', 'unknown')\n",
    "        task_type_match = expected_task_type == actual_task_type\n",
    "        \n",
    "        return {\n",
    "            'query_id': query_data.id,\n",
    "            'query': query_data.query,\n",
    "            'expected_task_type': expected_task_type,\n",
    "            'actual_task_type': actual_task_type,\n",
    "            'task_type_match': task_type_match,\n",
    "            'domain': query_data.domain,\n",
    "            'query_type': query_data.query_type,\n",
    "            'requires_current_info': query_data.requires_current_info,\n",
    "            'status': result.get('status', 'unknown'),\n",
    "            'response_length': response_length,\n",
    "            'citations_count': citations_count,\n",
    "            'total_tokens': total_tokens,\n",
    "            'execution_time': execution_time,\n",
    "            'quality_score': quality_score,\n",
    "            'citation_completeness': citation_completeness,\n",
    "            'agents_used': result.get('agents_used', []),\n",
    "            'model_used': result.get('model_used', 'unknown'),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'success': result.get('status') == 'success',\n",
    "            'max_time_seconds': query_data.max_time_seconds,\n",
    "            'within_time_limit': execution_time <= query_data.max_time_seconds\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        logger.error(f\"Evaluation failed for query {query_data.id}: {str(e)}\")\n",
    "        \n",
    "        return {\n",
    "            'query_id': query_data.id,\n",
    "            'query': query_data.query,\n",
    "            'expected_task_type': query_data.task_type.value if hasattr(query_data.task_type, 'value') else str(query_data.task_type),\n",
    "            'actual_task_type': 'error',\n",
    "            'task_type_match': False,\n",
    "            'domain': query_data.domain,\n",
    "            'query_type': query_data.query_type,\n",
    "            'requires_current_info': query_data.requires_current_info,\n",
    "            'status': 'error',\n",
    "            'response_length': 0,\n",
    "            'citations_count': 0,\n",
    "            'total_tokens': 0,\n",
    "            'execution_time': execution_time,\n",
    "            'quality_score': 0.0,\n",
    "            'citation_completeness': 0.0,\n",
    "            'agents_used': [],\n",
    "            'model_used': 'error',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'success': False,\n",
    "            'max_time_seconds': query_data.max_time_seconds,\n",
    "            'within_time_limit': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "async def run_evaluation_batch(queries: List[EvalQuery], session_id: str) -> List[Dict]:\n",
    "    \"\"\"Run evaluation on a batch of queries with progress tracking.\"\"\"\n",
    "    results = []\n",
    "    total = len(queries)\n",
    "    \n",
    "    with status_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"üöÄ Starting evaluation of {total} queries...\")\n",
    "        print(\"üìä User-Centric Architecture: Analyzing user intent for each query\")\n",
    "    \n",
    "    for i, query_data in enumerate(queries):\n",
    "        # Update progress\n",
    "        progress_bar.value = int((i / total) * 100)\n",
    "        \n",
    "        with status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"üìù Processing query {i+1}/{total}\")\n",
    "            print(f\"üéØ Expected Task Type: {query_data.task_type.value if hasattr(query_data.task_type, 'value') else str(query_data.task_type)}\")\n",
    "            print(f\"üí¨ Query: {query_data.query[:50]}...\")\n",
    "        \n",
    "        # Run single evaluation\n",
    "        result = await run_single_evaluation(query_data, session_id)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Brief pause to prevent overwhelming the API\n",
    "        await asyncio.sleep(0.5)\n",
    "    \n",
    "    progress_bar.value = 100\n",
    "    with status_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"‚úÖ Evaluation complete! Processed {len(results)} queries\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"üìä Evaluation functions defined (Updated for User-Centric Architecture)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Button click handler\n",
    "async def on_run_button_clicked(b):\n",
    "    \"\"\"Handle run evaluation button click.\"\"\"\n",
    "    global evaluation_results, current_evaluation_session\n",
    "    \n",
    "    # Get filter settings\n",
    "    max_queries = max_queries_slider.value\n",
    "    selected_task_types = list(task_type_filter.value)  # Changed from complexity\n",
    "    selected_domain = domain_filter.value\n",
    "    \n",
    "    # Filter queries based on settings (updated for user-centric architecture)\n",
    "    filtered_queries = []\n",
    "    for query in EVALUATION_QUERIES:\n",
    "        # Filter by task type (replaces complexity filter)\n",
    "        query_task_type = query.task_type.value if hasattr(query.task_type, 'value') else str(query.task_type)\n",
    "        if query_task_type not in selected_task_types:\n",
    "            continue\n",
    "            \n",
    "        # Filter by domain\n",
    "        if selected_domain != 'All' and query.domain != selected_domain:\n",
    "            continue\n",
    "            \n",
    "        filtered_queries.append(query)\n",
    "        \n",
    "        # Limit to max queries\n",
    "        if len(filtered_queries) >= max_queries:\n",
    "            break\n",
    "    \n",
    "    if not filtered_queries:\n",
    "        with status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"‚ö†Ô∏è No queries match the selected filters!\")\n",
    "        return\n",
    "    \n",
    "    # Start new evaluation session\n",
    "    eval_session_id = f\"interactive_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    current_evaluation_session = eval_session_id\n",
    "    \n",
    "    # Run evaluation\n",
    "    try:\n",
    "        run_button.disabled = True\n",
    "        run_button.description = '‚è≥ Running...'\n",
    "        \n",
    "        results = await run_evaluation_batch(filtered_queries, eval_session_id)\n",
    "        evaluation_results.extend(results)\n",
    "        \n",
    "        # Show comprehensive summary (updated for user-centric metrics)\n",
    "        successful = len([r for r in results if r['success']])\n",
    "        failed = len(results) - successful\n",
    "        avg_time = sum(r['execution_time'] for r in results) / len(results)\n",
    "        total_tokens = sum(r['total_tokens'] for r in results)\n",
    "        \n",
    "        # Task type accuracy\n",
    "        task_type_matches = len([r for r in results if r.get('task_type_match', False)])\n",
    "        task_type_accuracy = task_type_matches / len(results) if results else 0\n",
    "        \n",
    "        # Time limit compliance\n",
    "        within_time = len([r for r in results if r.get('within_time_limit', False)])\n",
    "        time_compliance = within_time / len(results) if results else 0\n",
    "        \n",
    "        with status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"üéâ User-Centric Evaluation Complete!\")\n",
    "            print(f\"‚úÖ Successful: {successful}\")\n",
    "            print(f\"‚ùå Failed: {failed}\")\n",
    "            print(f\"‚è±Ô∏è Avg Time: {avg_time:.2f}s\")\n",
    "            print(f\"ü™ô Total Tokens: {total_tokens:,}\")\n",
    "            print(f\"üéØ Task Type Accuracy: {task_type_accuracy:.1%}\")\n",
    "            print(f\"‚è∞ Time Compliance: {time_compliance:.1%}\")\n",
    "            print(f\"üìä Results stored in evaluation_results\")\n",
    "        \n",
    "    finally:\n",
    "        run_button.disabled = False\n",
    "        run_button.description = 'üöÄ Run Evaluation'\n",
    "\n",
    "# Connect button to handler\n",
    "run_button.on_click(lambda b: asyncio.create_task(on_run_button_clicked(b)))\n",
    "\n",
    "print(\"üéÆ Button handler connected (Updated for User-Centric Architecture)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Quick Test Evaluation\n",
    "\n",
    "Let's run a quick test with a few sample queries to verify everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test evaluation\n",
    "test_queries = [\n",
    "    EvalQuery(\n",
    "        id='test_1',\n",
    "        query='What is artificial intelligence?',\n",
    "        task_type=TaskType.DIRECT_ANSWER,\n",
    "        expected_sources=2,\n",
    "        requires_current_info=False,\n",
    "        domain='technology',\n",
    "        max_time_seconds=10,\n",
    "        query_type='qa'\n",
    "    ),\n",
    "    EvalQuery(\n",
    "        id='test_2', \n",
    "        query='Explain the latest developments in quantum computing and their implications for cryptography',\n",
    "        task_type=TaskType.RESEARCH_REPORT,\n",
    "        expected_sources=5,\n",
    "        requires_current_info=True,\n",
    "        domain='technology',\n",
    "        max_time_seconds=30,\n",
    "        query_type='research'\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"üß™ Running quick test evaluation...\")\n",
    "print(\"üìä Testing User-Centric Task Routing:\")\n",
    "print(\"  Test 1: DIRECT_ANSWER - Should use supervisor directly\")\n",
    "print(\"  Test 2: RESEARCH_REPORT - Should orchestrate multiple agents\")\n",
    "\n",
    "test_session_id = f\"test_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "test_results = await run_evaluation_batch(test_queries, test_session_id)\n",
    "\n",
    "# Display test results with task type analysis\n",
    "print(\"\\nüìä Test Results (User-Centric Architecture):\")\n",
    "for result in test_results:\n",
    "    status_emoji = \"‚úÖ\" if result['success'] else \"‚ùå\"\n",
    "    task_match_emoji = \"üéØ\" if result.get('task_type_match', False) else \"‚ùå\"\n",
    "    print(f\"{status_emoji} {result['query_id']}: {result['execution_time']:.2f}s, {result['total_tokens']} tokens, {result['citations_count']} citations\")\n",
    "    print(f\"   {task_match_emoji} Task Type: Expected {result['expected_task_type']} ‚Üí Actual {result['actual_task_type']}\")\n",
    "\n",
    "successful_tests = len([r for r in test_results if r['success']])\n",
    "task_type_accurate = len([r for r in test_results if r.get('task_type_match', False)])\n",
    "print(f\"\\nüéØ Test completed! {successful_tests}/{len(test_results)} successful, {task_type_accurate}/{len(test_results)} task type accurate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results analysis functions\n",
    "def analyze_evaluation_results(results: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze evaluation results and compute comprehensive metrics.\"\"\"\n",
    "    if not results:\n",
    "        return {\"error\": \"No results to analyze\"}\n",
    "    \n",
    "    successful = [r for r in results if r['success']]\n",
    "    failed = [r for r in results if not r['success']]\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics = {\n",
    "        'total_queries': len(results),\n",
    "        'successful': len(successful),\n",
    "        'failed': len(failed),\n",
    "        'success_rate': len(successful) / len(results) if results else 0,\n",
    "    }\n",
    "    \n",
    "    # User-centric architecture metrics\n",
    "    task_type_matches = len([r for r in results if r.get('task_type_match', False)])\n",
    "    within_time_limit = len([r for r in results if r.get('within_time_limit', False)])\n",
    "    \n",
    "    metrics.update({\n",
    "        'task_type_accuracy': task_type_matches / len(results) if results else 0,\n",
    "        'time_compliance_rate': within_time_limit / len(results) if results else 0,\n",
    "    })\n",
    "    \n",
    "    if successful:\n",
    "        # Performance metrics\n",
    "        execution_times = [r['execution_time'] for r in successful]\n",
    "        token_counts = [r['total_tokens'] for r in successful]\n",
    "        response_lengths = [r['response_length'] for r in successful]\n",
    "        citation_counts = [r['citations_count'] for r in successful]\n",
    "        quality_scores = [r['quality_score'] for r in successful]\n",
    "        \n",
    "        metrics.update({\n",
    "            'avg_execution_time': sum(execution_times) / len(execution_times),\n",
    "            'median_execution_time': sorted(execution_times)[len(execution_times)//2],\n",
    "            'max_execution_time': max(execution_times),\n",
    "            'min_execution_time': min(execution_times),\n",
    "            'total_tokens': sum(token_counts),\n",
    "            'avg_tokens': sum(token_counts) / len(token_counts),\n",
    "            'avg_response_length': sum(response_lengths) / len(response_lengths),\n",
    "            'avg_citations': sum(citation_counts) / len(citation_counts),\n",
    "            'avg_quality_score': sum(quality_scores) / len(quality_scores),\n",
    "        })\n",
    "        \n",
    "        # Performance by task type (replaces complexity analysis)\n",
    "        by_task_type = {}\n",
    "        for result in successful:\n",
    "            task_type = result['expected_task_type']\n",
    "            if task_type not in by_task_type:\n",
    "                by_task_type[task_type] = []\n",
    "            by_task_type[task_type].append(result)\n",
    "        \n",
    "        task_type_metrics = {}\n",
    "        for task_type, type_results in by_task_type.items():\n",
    "            task_type_metrics[task_type] = {\n",
    "                'count': len(type_results),\n",
    "                'avg_time': sum(r['execution_time'] for r in type_results) / len(type_results),\n",
    "                'avg_tokens': sum(r['total_tokens'] for r in type_results) / len(type_results),\n",
    "                'avg_quality': sum(r['quality_score'] for r in type_results) / len(type_results),\n",
    "                'task_type_accuracy': len([r for r in type_results if r.get('task_type_match', False)]) / len(type_results),\n",
    "                'time_compliance': len([r for r in type_results if r.get('within_time_limit', False)]) / len(type_results),\n",
    "            }\n",
    "        \n",
    "        metrics['by_task_type'] = task_type_metrics\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def visualize_results(results: List[Dict]):\n",
    "    \"\"\"Create comprehensive visualizations of evaluation results.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('User-Centric Multi-Agent System Evaluation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Success Rate\n",
    "    success_counts = df['success'].value_counts()\n",
    "    axes[0,0].pie(success_counts.values, labels=['Success', 'Failed'], autopct='%1.1f%%', \n",
    "                  colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
    "    axes[0,0].set_title('Overall Success Rate')\n",
    "    \n",
    "    # 2. Task Type Accuracy\n",
    "    successful_df = df[df['success'] == True]\n",
    "    if not successful_df.empty and 'task_type_match' in successful_df.columns:\n",
    "        task_type_accuracy = successful_df['task_type_match'].value_counts()\n",
    "        axes[0,1].pie(task_type_accuracy.values, labels=['Correct', 'Incorrect'], autopct='%1.1f%%',\n",
    "                     colors=['#3498db', '#f39c12'], startangle=90)\n",
    "        axes[0,1].set_title('Task Type Routing Accuracy')\n",
    "    \n",
    "    # 3. Execution Time Distribution\n",
    "    if not successful_df.empty:\n",
    "        axes[0,2].hist(successful_df['execution_time'], bins=15, alpha=0.7, color='#9b59b6')\n",
    "        axes[0,2].set_xlabel('Execution Time (seconds)')\n",
    "        axes[0,2].set_ylabel('Frequency')\n",
    "        axes[0,2].set_title('Execution Time Distribution')\n",
    "        axes[0,2].axvline(successful_df['execution_time'].mean(), color='red', linestyle='--', \n",
    "                         label=f'Mean: {successful_df[\"execution_time\"].mean():.2f}s')\n",
    "        axes[0,2].legend()\n",
    "    \n",
    "    # 4. Token Usage by Task Type\n",
    "    if not successful_df.empty and 'expected_task_type' in successful_df.columns:\n",
    "        task_type_tokens = successful_df.groupby('expected_task_type')['total_tokens'].mean()\n",
    "        colors = ['#3498db', '#e74c3c', '#2ecc71']  # Blue, Red, Green\n",
    "        axes[1,0].bar(task_type_tokens.index, task_type_tokens.values, color=colors[:len(task_type_tokens)])\n",
    "        axes[1,0].set_xlabel('Task Type')\n",
    "        axes[1,0].set_ylabel('Average Tokens')\n",
    "        axes[1,0].set_title('Token Usage by Task Type')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 5. Response Length vs Citations (colored by task type)\n",
    "    if not successful_df.empty and 'expected_task_type' in successful_df.columns:\n",
    "        task_type_colors = {'direct_answer': 'blue', 'search_needed': 'red', 'research_report': 'green'}\n",
    "        for task_type in successful_df['expected_task_type'].unique():\n",
    "            task_data = successful_df[successful_df['expected_task_type'] == task_type]\n",
    "            color = task_type_colors.get(task_type, 'gray')\n",
    "            axes[1,1].scatter(task_data['response_length'], task_data['citations_count'], \n",
    "                            c=color, alpha=0.6, label=task_type)\n",
    "        axes[1,1].set_xlabel('Response Length (characters)')\n",
    "        axes[1,1].set_ylabel('Number of Citations')\n",
    "        axes[1,1].set_title('Response Length vs Citations by Task Type')\n",
    "        axes[1,1].legend()\n",
    "    \n",
    "    # 6. Time Compliance by Task Type\n",
    "    if not successful_df.empty and 'expected_task_type' in successful_df.columns:\n",
    "        time_compliance = successful_df.groupby('expected_task_type')['within_time_limit'].mean()\n",
    "        colors = ['#3498db', '#e74c3c', '#2ecc71']  \n",
    "        bars = axes[1,2].bar(time_compliance.index, time_compliance.values, color=colors[:len(time_compliance)])\n",
    "        axes[1,2].set_ylabel('Time Compliance Rate')\n",
    "        axes[1,2].set_xlabel('Task Type')\n",
    "        axes[1,2].set_title('Time Limit Compliance by Task Type')\n",
    "        axes[1,2].tick_params(axis='x', rotation=45)\n",
    "        axes[1,2].set_ylim(0, 1)\n",
    "        \n",
    "        # Add percentage labels on bars\n",
    "        for bar, value in zip(bars, time_compliance.values):\n",
    "            axes[1,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                          f'{value:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"üìä Analysis and visualization functions ready (Updated for User-Centric Architecture)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and visualize current results\n",
    "if evaluation_results:\n",
    "    print(\"üìà Analyzing Current Results (User-Centric Architecture)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    metrics = analyze_evaluation_results(evaluation_results)\n",
    "    \n",
    "    print(f\"üìä Total Queries: {metrics['total_queries']}\")\n",
    "    print(f\"‚úÖ Successful: {metrics['successful']} ({metrics['success_rate']:.1%})\")\n",
    "    print(f\"‚ùå Failed: {metrics['failed']}\")\n",
    "    print(f\"üéØ Task Type Accuracy: {metrics.get('task_type_accuracy', 0):.1%}\")\n",
    "    print(f\"‚è∞ Time Compliance Rate: {metrics.get('time_compliance_rate', 0):.1%}\")\n",
    "    \n",
    "    if metrics['successful'] > 0:\n",
    "        print(f\"‚è±Ô∏è Avg Execution Time: {metrics['avg_execution_time']:.2f}s\")\n",
    "        print(f\"ü™ô Total Tokens: {metrics['total_tokens']:,}\")\n",
    "        print(f\"üìù Avg Response Length: {metrics['avg_response_length']:.0f} chars\")\n",
    "        print(f\"üìö Avg Citations: {metrics['avg_citations']:.1f}\")\n",
    "        print(f\"‚≠ê Avg Quality Score: {metrics['avg_quality_score']:.2f}\")\n",
    "        \n",
    "        if 'by_task_type' in metrics:\n",
    "            print(\"\\nüìã Performance by Task Type:\")\n",
    "            for task_type, type_metrics in metrics['by_task_type'].items():\n",
    "                print(f\"  {task_type}:\")\n",
    "                print(f\"    Count: {type_metrics['count']}\")\n",
    "                print(f\"    Avg Time: {type_metrics['avg_time']:.2f}s\")\n",
    "                print(f\"    Avg Tokens: {type_metrics['avg_tokens']:.0f}\")\n",
    "                print(f\"    Task Type Accuracy: {type_metrics['task_type_accuracy']:.1%}\")\n",
    "                print(f\"    Time Compliance: {type_metrics['time_compliance']:.1%}\")\n",
    "    \n",
    "    # Show visualizations\n",
    "    print(\"\\nüìä Generating User-Centric Architecture Visualizations...\")\n",
    "    visualize_results(evaluation_results)\n",
    "    \n",
    "else:\n",
    "    print(\"üìù No evaluation results yet. Use the controls above to run an evaluation!\")\n",
    "    print(\"üéØ The system now uses User-Centric Architecture:\")\n",
    "    print(\"  - Analyzes user intent, not just query complexity\")\n",
    "    print(\"  - Routes to appropriate response strategy\")\n",
    "    print(\"  - Measures task type routing accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export functions\n",
    "def export_results_to_csv(results: List[Dict], filename: str = None):\n",
    "    \"\"\"Export evaluation results to CSV file.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to export\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"üìÅ Results exported to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def export_results_to_json(results: List[Dict], filename: str = None):\n",
    "    \"\"\"Export evaluation results to JSON file.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to export\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üìÅ Results exported to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Export widgets\n",
    "export_csv_button = widgets.Button(\n",
    "    description='üíæ Export CSV',\n",
    "    button_style='info',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "export_json_button = widgets.Button(\n",
    "    description='üíæ Export JSON',\n",
    "    button_style='info', \n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "def on_export_csv_clicked(b):\n",
    "    if evaluation_results:\n",
    "        export_results_to_csv(evaluation_results)\n",
    "    else:\n",
    "        print(\"No results to export\")\n",
    "\n",
    "def on_export_json_clicked(b):\n",
    "    if evaluation_results:\n",
    "        export_results_to_json(evaluation_results)\n",
    "    else:\n",
    "        print(\"No results to export\")\n",
    "\n",
    "export_csv_button.on_click(on_export_csv_clicked)\n",
    "export_json_button.on_click(on_export_json_clicked)\n",
    "\n",
    "print(\"üíæ Export Controls\")\n",
    "display(widgets.HBox([export_csv_button, export_json_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Custom Query Testing\n",
    "\n",
    "Test the multi-agent system with your own custom queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom query testing interface\n",
    "custom_query_text = widgets.Textarea(\n",
    "    placeholder='Enter your custom query here...',\n",
    "    description='Custom Query:',\n",
    "    layout={'width': '100%', 'height': '100px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "test_button = widgets.Button(\n",
    "    description='üß™ Test Query',\n",
    "    button_style='warning',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "custom_output = widgets.Output()\n",
    "\n",
    "async def test_custom_query(query: str):\n",
    "    \"\"\"Test a custom query through the multi-agent system.\"\"\"\n",
    "    if not query.strip():\n",
    "        print(\"Please enter a query to test\")\n",
    "        return\n",
    "    \n",
    "    with custom_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"üß™ Testing query: {query}\")\n",
    "        print(\"‚è≥ Processing...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = await system.process_query(\n",
    "            query=query,\n",
    "            trace_id=f\"custom_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            session_id=\"custom_testing\"\n",
    "        )\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        with custom_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"üß™ Query: {query}\")\n",
    "            print(f\"‚úÖ Status: {result.get('status', 'unknown')}\")\n",
    "            print(f\"‚è±Ô∏è Execution Time: {execution_time:.2f}s\")\n",
    "            print(f\"ü™ô Total Tokens: {result.get('total_tokens', 0):,}\")\n",
    "            print(f\"üìö Citations: {len(result.get('citations', []))}\")\n",
    "            print(f\"ü§ñ Agents Used: {', '.join(result.get('agents_used', []))}\")\n",
    "            print(\"\\nüìÑ Response:\")\n",
    "            print(\"-\" * 50)\n",
    "            response = result.get('response', 'No response available')\n",
    "            if isinstance(response, dict) and 'extracted_content' in response:\n",
    "                print(response['extracted_content'][:500] + \"...\" if len(response['extracted_content']) > 500 else response['extracted_content'])\n",
    "            else:\n",
    "                print(str(response)[:500] + \"...\" if len(str(response)) > 500 else str(response))\n",
    "        \n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        with custom_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"‚ùå Error testing query: {str(e)}\")\n",
    "            print(f\"‚è±Ô∏è Time before error: {execution_time:.2f}s\")\n",
    "\n",
    "async def on_test_button_clicked(b):\n",
    "    query = custom_query_text.value\n",
    "    await test_custom_query(query)\n",
    "\n",
    "test_button.on_click(lambda b: asyncio.create_task(on_test_button_clicked(b)))\n",
    "\n",
    "print(\"üß™ Custom Query Testing\")\n",
    "display(widgets.VBox([\n",
    "    custom_query_text,\n",
    "    test_button,\n",
    "    custom_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup and Session Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup functions\n",
    "def clear_results():\n",
    "    \"\"\"Clear all evaluation results.\"\"\"\n",
    "    global evaluation_results\n",
    "    evaluation_results = []\n",
    "    print(\"üßπ Evaluation results cleared\")\n",
    "\n",
    "async def close_phoenix_session():\n",
    "    \"\"\"Close the current Phoenix session.\"\"\"\n",
    "    try:\n",
    "        final_metrics = phoenix_integration.close_session()\n",
    "        print(f\"üî• Phoenix session closed: {final_metrics}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error closing Phoenix session: {str(e)}\")\n",
    "\n",
    "async def system_shutdown():\n",
    "    \"\"\"Gracefully shutdown the multi-agent system.\"\"\"\n",
    "    try:\n",
    "        await system.shutdown()\n",
    "        print(\"ü§ñ Multi-agent system shutdown complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error during system shutdown: {str(e)}\")\n",
    "\n",
    "# Cleanup buttons\n",
    "clear_button = widgets.Button(\n",
    "    description='üßπ Clear Results',\n",
    "    button_style='danger',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "close_session_button = widgets.Button(\n",
    "    description='üî• Close Phoenix',\n",
    "    button_style='warning',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "shutdown_button = widgets.Button(\n",
    "    description='üõë Shutdown System',\n",
    "    button_style='danger',\n",
    "    layout={'width': '150px'}\n",
    ")\n",
    "\n",
    "clear_button.on_click(lambda b: clear_results())\n",
    "close_session_button.on_click(lambda b: asyncio.create_task(close_phoenix_session()))\n",
    "shutdown_button.on_click(lambda b: asyncio.create_task(system_shutdown()))\n",
    "\n",
    "print(\"üßπ Cleanup Controls (Updated for Direct Phoenix SDK)\")\n",
    "display(widgets.HBox([clear_button, close_session_button, shutdown_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Usage Instructions (Updated for User-Centric Architecture)\n",
    "\n",
    "### üöÄ Getting Started\n",
    "1. **Initialize**: Run the setup cells to initialize the user-centric multi-agent system\n",
    "2. **Configure**: Use the interactive controls to set evaluation parameters  \n",
    "3. **Run**: Click \"üöÄ Run Evaluation\" to start the evaluation process\n",
    "4. **Analyze**: View results and user-centric architecture visualizations\n",
    "5. **Export**: Save results to CSV or JSON for further analysis\n",
    "\n",
    "### üéØ User-Centric Architecture Features\n",
    "- **Task Type Routing**: System analyzes user intent (DIRECT_ANSWER, SEARCH_NEEDED, RESEARCH_REPORT)\n",
    "- **Autonomous Model Selection**: Each agent chooses optimal GPT-5 variant per query\n",
    "- **Direct Phoenix SDK**: Reliable tracing without MCP issues  \n",
    "- **100% Test Coverage**: All system components thoroughly tested\n",
    "\n",
    "### üéÆ Interactive Controls\n",
    "- **Max Queries**: Limit the number of queries to evaluate\n",
    "- **Task Type Filter**: Select which user intent types to test (replaces complexity filter)\n",
    "- **Domain Filter**: Focus on specific domains or test all\n",
    "- **Progress Bar**: Real-time progress tracking during evaluation\n",
    "\n",
    "### üß™ Custom Testing  \n",
    "- Use the \"Custom Query Testing\" section to test individual queries\n",
    "- Great for debugging task type routing behavior\n",
    "- Observe how the system analyzes user intent\n",
    "\n",
    "### üìä Results Analysis (Enhanced)\n",
    "- **Success Rate**: Overall system reliability\n",
    "- **Task Type Accuracy**: How well the system identifies user intent\n",
    "- **Time Compliance**: Adherence to performance targets\n",
    "- **Performance by Task Type**: Detailed breakdown of routing effectiveness\n",
    "- Visual analysis with task type-specific charts\n",
    "\n",
    "### üî• Phoenix Integration (Direct SDK)\n",
    "- Real-time tracing and observability via direct SDK\n",
    "- Visit http://localhost:6006 to view Phoenix UI  \n",
    "- No more MCP integration issues\n",
    "- Session management and cleanup functions\n",
    "\n",
    "### üí° Tips for User-Centric Evaluation\n",
    "- Test all three task types to validate routing accuracy\n",
    "- Monitor task type accuracy metrics closely\n",
    "- Compare performance across different user intent categories\n",
    "- Use custom queries to test edge cases in intent analysis\n",
    "- Export results regularly for backup and analysis\n",
    "\n",
    "### üéØ Key Metrics to Watch\n",
    "- **Task Type Accuracy**: Should be >90% for good intent analysis\n",
    "- **Time Compliance**: Measures adherence to performance targets\n",
    "- **Token Efficiency**: User-centric routing should reduce token usage\n",
    "- **Response Appropriateness**: Different response strategies for different intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final status check\n",
    "print(\"üéâ Multi-Agent Evaluation Notebook Ready! (User-Centric Architecture)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"ü§ñ System Status: {'‚úÖ Active' if system.is_initialized else '‚ùå Inactive'}\")\n",
    "print(f\"üî• Phoenix Session: {session_id}\")\n",
    "print(f\"üìä Dataset Size: {len(EVALUATION_QUERIES)} queries\")\n",
    "print(f\"üìà Current Results: {len(evaluation_results)} evaluations\")\n",
    "print()\n",
    "print(\"üéØ User-Centric Architecture Features:\")\n",
    "print(\"  ‚úÖ Task Type Routing: DIRECT_ANSWER, SEARCH_NEEDED, RESEARCH_REPORT\")\n",
    "print(\"  ‚úÖ Autonomous Model Selection: Each agent optimizes model choice\")\n",
    "print(\"  ‚úÖ Direct Phoenix SDK: No more MCP integration issues\")\n",
    "print(\"  ‚úÖ 100% Test Coverage: Fully tested and reliable\")\n",
    "print()\n",
    "print(\"üìä Key Evaluation Metrics:\")\n",
    "print(\"  ‚Ä¢ Task Type Accuracy: How well user intent is identified\")\n",
    "print(\"  ‚Ä¢ Time Compliance: Performance target adherence\")\n",
    "print(\"  ‚Ä¢ Token Efficiency: User-centric routing optimization\")\n",
    "print(\"  ‚Ä¢ Response Quality: Appropriate responses for different intents\")\n",
    "print()\n",
    "print(\"üöÄ Ready to run user-centric evaluations!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
