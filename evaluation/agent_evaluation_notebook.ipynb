{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Research System Evaluation\n",
    "\n",
    "This notebook provides comprehensive testing and evaluation of the multi-agent research system using the 40-query evaluation dataset and Arize Phoenix integration.\n",
    "\n",
    "## Overview\n",
    "- **Dataset**: 40 queries across 4 complexity levels (Simple, Moderate, Complex)\n",
    "- **Query Types**: Q&A (30 queries) + Deep Research (10 queries)\n",
    "- **Evaluation**: Automated scoring via Arize Phoenix evaluators\n",
    "- **Metrics**: Response quality, latency, token usage, citation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install pandas numpy matplotlib seaborn plotly\n",
    "!pip install arize-phoenix openai\n",
    "!pip install asyncio aiohttp tenacity\n",
    "!pip install jupyter-widgets ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# System imports\nimport asyncio\nimport time\nimport json\nfrom datetime import datetime\nfrom typing import List, Dict, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Arize Phoenix imports\nimport phoenix as px\nfrom phoenix.evals import (\n    HallucinationEvaluator,\n    RelevanceEvaluator, \n    QACorrectnessEvaluator,\n    run_evals\n)\n\n# Project imports (adjust path as needed)\nimport sys\nsys.path.append('..')\n\nfrom evaluation.evaluation_dataset import (\n    to_pandas, to_csv, to_arize_format, create_evaluation_template,\n    get_queries_by_complexity, get_queries_by_type, EVALUATION_QUERIES\n)\n\n# Agent imports - Complete multi-agent system\ntry:\n    from agents.supervisor import SupervisorAgent\n    from agents.search import SearchAgent\n    from agents.citation import CitationAgent\n    from agents.multi_agents import MultiAgentResearchSystem, initialize_system\n    AGENTS_AVAILABLE = True\n    print(\"✅ Complete agent system imported successfully\")\nexcept ImportError as e:\n    AGENTS_AVAILABLE = False\n    print(f\"⚠️ Agent modules not yet available: {e}\")\n    print(\"📝 This notebook will demonstrate evaluation setup and mock agent responses\")\n\nprint(\"📚 All dependencies loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evaluation dataset\n",
    "eval_df = to_pandas()\n",
    "print(f\"📊 Loaded {len(eval_df)} evaluation queries\")\n",
    "print(f\"Columns: {list(eval_df.columns)}\")\n",
    "print(f\"Shape: {eval_df.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics and distribution\n",
    "print(\"🔍 Dataset Overview:\")\n",
    "print(f\"Total queries: {len(eval_df)}\")\n",
    "print(f\"Unique domains: {eval_df['domain'].nunique()}\")\n",
    "print(f\"Query types: {eval_df['query_type'].value_counts().to_dict()}\")\n",
    "print(f\"Complexity levels: {eval_df['complexity'].value_counts().to_dict()}\")\n",
    "print(f\"Require current info: {eval_df['requires_current_info'].sum()}/{len(eval_df)}\")\n",
    "print(f\"Average expected sources: {eval_df['expected_sources'].mean():.1f}\")\n",
    "print(f\"Average max time: {eval_df['max_time_seconds'].mean():.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Complexity distribution\n",
    "eval_df['complexity'].value_counts().plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('Distribution by Complexity Level')\n",
    "axes[0,0].set_xlabel('Complexity')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "\n",
    "# Query type distribution\n",
    "eval_df['query_type'].value_counts().plot(kind='pie', ax=axes[0,1], autopct='%1.1f%%')\n",
    "axes[0,1].set_title('Distribution by Query Type')\n",
    "\n",
    "# Domain distribution (top 10)\n",
    "eval_df['domain'].value_counts().head(10).plot(kind='barh', ax=axes[1,0], color='lightcoral')\n",
    "axes[1,0].set_title('Top 10 Domains')\n",
    "axes[1,0].set_xlabel('Count')\n",
    "\n",
    "# Expected sources vs complexity\n",
    "sns.boxplot(data=eval_df, x='complexity', y='expected_sources', ax=axes[1,1])\n",
    "axes[1,1].set_title('Expected Sources by Complexity')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class AgentEvaluationFramework:\n    \"\"\"Framework for testing and evaluating multi-agent system\"\"\"\n    \n    def __init__(self):\n        self.results = []\n        self.evaluation_df = None\n        self.phoenix_session = None\n        self.research_system = None\n        \n    def setup_phoenix(self, launch_app=True):\n        \"\"\"Initialize Phoenix for evaluation tracking\"\"\"\n        if launch_app:\n            self.phoenix_session = px.launch_app()\n            print(\"🔥 Phoenix app launched at http://localhost:6006\")\n        else:\n            print(\"📊 Phoenix tracking enabled (no UI)\")\n    \n    def initialize_research_system(self):\n        \"\"\"Initialize the multi-agent research system\"\"\"\n        if AGENTS_AVAILABLE and self.research_system is None:\n            try:\n                self.research_system = initialize_system()\n                print(\"🤖 Multi-agent research system initialized\")\n                return True\n            except Exception as e:\n                print(f\"❌ Failed to initialize research system: {e}\")\n                return False\n        elif self.research_system is not None:\n            print(\"🤖 Research system already initialized\")\n            return True\n        else:\n            print(\"⚠️ Agents not available, using mock mode\")\n            return False\n    \n    async def test_single_query(self, query_data: Dict, mock_mode=False) -> Dict:\n        \"\"\"Test a single query against the agent system\"\"\"\n        start_time = time.time()\n        \n        if mock_mode or not AGENTS_AVAILABLE or self.research_system is None:\n            # Mock agent response for demonstration\n            await asyncio.sleep(np.random.uniform(0.5, 3.0))  # Simulate processing time\n            \n            response = {\n                'query_id': query_data['id'],\n                'input': query_data['query'],\n                'output': self._generate_mock_response(query_data),\n                'complexity': query_data['complexity'],\n                'domain': query_data['domain'],\n                'query_type': query_data['query_type'],\n                'expected_sources': query_data['expected_sources'],\n                'response_time_ms': (time.time() - start_time) * 1000,\n                'tokens_used': np.random.randint(100, 1000),\n                'model_used': self._get_mock_model(query_data['complexity']),\n                'sources_found': np.random.randint(1, query_data['expected_sources'] + 2),\n                'citations_count': np.random.randint(1, 5),\n                'timestamp': datetime.now()\n            }\n        else:\n            # Real agent testing using the multi-agent system\n            try:\n                result = await self.research_system.process_query(query_data['query'])\n                execution_time_ms = (time.time() - start_time) * 1000\n                \n                # Extract data from real agent response\n                agent_response = result.get('response', '')\n                citations = result.get('citations', [])\n                \n                response = {\n                    'query_id': query_data['id'],\n                    'input': query_data['query'],\n                    'output': agent_response,\n                    'complexity': query_data['complexity'],\n                    'domain': query_data['domain'],\n                    'query_type': query_data['query_type'],\n                    'expected_sources': query_data['expected_sources'],\n                    'response_time_ms': execution_time_ms,\n                    'tokens_used': result.get('total_tokens', 0),\n                    'model_used': result.get('model_used', 'gpt-5'),\n                    'sources_found': len(citations),\n                    'citations_count': len(citations),\n                    'timestamp': datetime.now(),\n                    'session_id': result.get('session_id', ''),\n                    'trace_id': result.get('trace_id', ''),\n                    'status': result.get('status', 'completed')\n                }\n                \n                # Add citation details if available\n                if citations:\n                    response['citation_details'] = [\n                        {\n                            'url': c.url if hasattr(c, 'url') else str(c),\n                            'title': c.title if hasattr(c, 'title') else 'Unknown',\n                            'credibility_score': c.credibility_score if hasattr(c, 'credibility_score') else 0.5\n                        }\n                        for c in citations[:5]  # Limit to top 5 citations\n                    ]\n                \n            except Exception as e:\n                print(f\"❌ Real agent test failed for query {query_data['id']}: {str(e)}\")\n                # Fallback to mock response on error\n                execution_time_ms = (time.time() - start_time) * 1000\n                response = {\n                    'query_id': query_data['id'],\n                    'input': query_data['query'],\n                    'output': f\"Error processing query: {str(e)}\",\n                    'complexity': query_data['complexity'],\n                    'domain': query_data['domain'],\n                    'query_type': query_data['query_type'],\n                    'expected_sources': query_data['expected_sources'],\n                    'response_time_ms': execution_time_ms,\n                    'tokens_used': 0,\n                    'model_used': 'error',\n                    'sources_found': 0,\n                    'citations_count': 0,\n                    'timestamp': datetime.now(),\n                    'error': str(e),\n                    'status': 'failed'\n                }\n        \n        return response\n    \n    def _generate_mock_response(self, query_data: Dict) -> str:\n        \"\"\"Generate realistic mock responses based on query complexity\"\"\"\n        complexity = query_data['complexity']\n        query_type = query_data['query_type']\n        \n        if complexity == 'gpt-5-nano':  # Simple\n            return f\"Mock simple answer to: {query_data['query'][:50]}... [This would be a direct, factual response of 1-2 sentences]\"\n        elif complexity == 'gpt-5-mini':  # Moderate  \n            return f\"Mock moderate answer to: {query_data['query'][:50]}... [This would be a comprehensive explanation with 2-3 paragraphs covering key concepts and examples]\"\n        else:  # Complex\n            if query_type == 'research':\n                return f\"Mock comprehensive research report on: {query_data['query'][:50]}... [This would be a 2-page detailed analysis with multiple sections, current data, examples, and projections]\"\n            else:\n                return f\"Mock complex analysis of: {query_data['query'][:50]}... [This would be an in-depth technical explanation with current developments, multiple perspectives, and detailed examples]\"\n    \n    def _get_mock_model(self, complexity: str) -> str:\n        \"\"\"Return appropriate model based on complexity\"\"\"\n        model_map = {\n            'gpt-5-nano': 'gpt-5-nano',\n            'gpt-5-mini': 'gpt-5-mini', \n            'gpt-5': 'gpt-5'\n        }\n        return model_map.get(complexity, 'gpt-5')\n    \n    async def run_batch_evaluation(self, queries_subset=None, mock_mode=False, max_concurrent=3):\n        \"\"\"Run evaluation on a batch of queries\"\"\"\n        if queries_subset is None:\n            queries_subset = EVALUATION_QUERIES\n        \n        # Initialize research system if using real agents\n        if not mock_mode and AGENTS_AVAILABLE:\n            system_ready = self.initialize_research_system()\n            if not system_ready:\n                print(\"⚠️ Falling back to mock mode due to system initialization failure\")\n                mock_mode = True\n        \n        print(f\"🚀 Starting batch evaluation of {len(queries_subset)} queries\")\n        print(f\"   Mode: {'Mock' if mock_mode else 'Real Agents'}, Max concurrent: {max_concurrent}\")\n        \n        # Create semaphore to limit concurrency\n        semaphore = asyncio.Semaphore(max_concurrent)\n        \n        async def bounded_test(query):\n            async with semaphore:\n                return await self.test_single_query(query.dict(), mock_mode)\n        \n        # Run tests concurrently with progress tracking\n        tasks = [bounded_test(query) for query in queries_subset]\n        self.results = []\n        \n        for i, task in enumerate(asyncio.as_completed(tasks)):\n            result = await task\n            self.results.append(result)\n            if len(self.results) % 5 == 0 or len(self.results) == len(queries_subset):\n                print(f\"   ✅ Completed {len(self.results)}/{len(queries_subset)} queries\")\n        \n        print(f\"🎉 Batch evaluation completed! {len(self.results)} responses collected\")\n        \n        # Show success/failure summary\n        successful = len([r for r in self.results if r.get('status', 'completed') != 'failed'])\n        if successful < len(self.results):\n            failed = len(self.results) - successful\n            print(f\"   ⚠️ {successful} successful, {failed} failed\")\n        \n        # Convert to DataFrame for analysis\n        self.evaluation_df = pd.DataFrame(self.results)\n        return self.evaluation_df\n    \n    def save_results(self, filepath: str):\n        \"\"\"Save evaluation results to CSV\"\"\"\n        if self.evaluation_df is not None:\n            self.evaluation_df.to_csv(filepath, index=False)\n            print(f\"💾 Results saved to {filepath}\")\n        else:\n            print(\"❌ No results to save. Run evaluation first.\")\n    \n    def get_system_stats(self):\n        \"\"\"Get system statistics if real system is available\"\"\"\n        if self.research_system is not None:\n            return self.research_system.get_system_stats()\n        else:\n            return {\"message\": \"Real system not available\"}\n\n# Initialize the evaluation framework\nevaluator = AgentEvaluationFramework()\nprint(\"🔧 Agent evaluation framework initialized\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Test with Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with a small subset first (one from each complexity level)\nsample_queries = [\n    EVALUATION_QUERIES[0],  # Simple\n    EVALUATION_QUERIES[10], # Moderate \n    EVALUATION_QUERIES[20], # Complex Q&A\n    EVALUATION_QUERIES[30], # Deep Research\n]\n\nprint(\"🧪 Running quick test with 4 sample queries...\")\nprint(\"   First testing with real agents if available, then mock mode for comparison\")\n\n# Try real agents first\nsample_results = await evaluator.run_batch_evaluation(sample_queries, mock_mode=False, max_concurrent=2)\n\n# Display results\ndisplay_cols = ['query_id', 'complexity', 'domain', 'response_time_ms', 'tokens_used', 'sources_found', 'status']\nprint(\"\\n📊 Sample Results:\")\nsample_results[display_cols].round(2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full Evaluation Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run full evaluation with real agents if available\nprint(\"🚀 Starting FULL evaluation of all 40 queries...\")\nprint(\"⏱️ This may take 10-20 minutes with real agents (vs 5 minutes with mock)\")\n\n# First check if agents are available and working\nif AGENTS_AVAILABLE:\n    print(\"🤖 Attempting to use real multi-agent system...\")\n    full_results = await evaluator.run_batch_evaluation(EVALUATION_QUERIES, mock_mode=False, max_concurrent=3)\nelse:\n    print(\"🎭 Using mock mode for demonstration...\")\n    full_results = await evaluator.run_batch_evaluation(EVALUATION_QUERIES, mock_mode=True, max_concurrent=5)\n\nprint(f\"\\n🎉 Full evaluation completed! {len(full_results)} responses generated\")\n\n# Show system statistics if real system was used\nif not full_results.empty and evaluator.research_system is not None:\n    print(\"\\n🔍 Real System Statistics:\")\n    stats = evaluator.get_system_stats()\n    if 'system_info' in stats:\n        print(f\"  System Version: {stats['system_info']['version']}\")\n        print(f\"  Agents Count: {stats['system_info']['agents_count']}\")\n        print(f\"  Success Rate: {stats['session_stats']['success_rate']:.2%}\")\n        print(f\"  Total Sessions: {stats['session_stats']['total_sessions']}\")\n    else:\n        print(\"  System stats:\", stats)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for future analysis\n",
    "evaluator.save_results('agent_evaluation_results.csv')\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"📈 Evaluation Results Summary:\")\n",
    "print(f\"Total queries processed: {len(full_results)}\")\n",
    "print(f\"Average response time: {full_results['response_time_ms'].mean():.1f}ms\")\n",
    "print(f\"Average tokens used: {full_results['tokens_used'].mean():.0f}\")\n",
    "print(f\"Average sources found: {full_results['sources_found'].mean():.1f}\")\n",
    "print(f\"Model distribution: {full_results['model_used'].value_counts().to_dict()}\")\n",
    "\n",
    "# Show first few complete results\n",
    "full_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Arize Phoenix Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Phoenix evaluation\n",
    "evaluator.setup_phoenix(launch_app=True)\n",
    "\n",
    "# Prepare data for Phoenix evaluators\n",
    "phoenix_df = full_results.copy()\n",
    "\n",
    "# Add reference answers for comparison (in real scenario, these would be expert-verified)\n",
    "# For demo, we'll use simplified mock references\n",
    "phoenix_df['reference'] = phoenix_df.apply(lambda row: f\"Reference answer for {row['input'][:30]}...\", axis=1)\n",
    "\n",
    "print(\"🔍 Setting up Phoenix evaluators...\")\n",
    "print(f\"Prepared {len(phoenix_df)} responses for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phoenix evaluations\n",
    "try:\n",
    "    # Initialize evaluators\n",
    "    hallucination_evaluator = HallucinationEvaluator()\n",
    "    relevance_evaluator = RelevanceEvaluator()\n",
    "    qa_evaluator = QACorrectnessEvaluator()\n",
    "    \n",
    "    print(\"🔄 Running Phoenix evaluations...\")\n",
    "    \n",
    "    # Run evaluations (this requires the columns 'input', 'output', 'reference')\n",
    "    eval_results = run_evals(\n",
    "        dataframe=phoenix_df,\n",
    "        evaluators=[hallucination_evaluator, relevance_evaluator, qa_evaluator],\n",
    "        provide_explanation=True\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Phoenix evaluations completed!\")\n",
    "    \n",
    "    # Combine results\n",
    "    for i, eval_df in enumerate(eval_results):\n",
    "        evaluator_name = ['hallucination', 'relevance', 'qa_correctness'][i]\n",
    "        phoenix_df[f'{evaluator_name}_score'] = eval_df['score']\n",
    "        phoenix_df[f'{evaluator_name}_label'] = eval_df['label']\n",
    "        phoenix_df[f'{evaluator_name}_explanation'] = eval_df['explanation']\n",
    "    \n",
    "    print(\"📊 Evaluation scores added to results DataFrame\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Phoenix evaluation failed (likely due to API setup): {e}\")\n",
    "    print(\"💡 Adding mock evaluation scores for demonstration...\")\n",
    "    \n",
    "    # Add mock evaluation scores\n",
    "    np.random.seed(42)  # For reproducible mock data\n",
    "    phoenix_df['hallucination_score'] = np.random.uniform(0.7, 1.0, len(phoenix_df))\n",
    "    phoenix_df['relevance_score'] = np.random.uniform(0.6, 1.0, len(phoenix_df))\n",
    "    phoenix_df['qa_correctness_score'] = np.random.uniform(0.5, 0.95, len(phoenix_df))\n",
    "    \n",
    "    phoenix_df['hallucination_label'] = phoenix_df['hallucination_score'].apply(lambda x: 'factual' if x > 0.8 else 'hallucinated')\n",
    "    phoenix_df['relevance_label'] = phoenix_df['relevance_score'].apply(lambda x: 'relevant' if x > 0.7 else 'irrelevant')\n",
    "    phoenix_df['qa_correctness_label'] = phoenix_df['qa_correctness_score'].apply(lambda x: 'correct' if x > 0.7 else 'incorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis by complexity\n",
    "performance_by_complexity = phoenix_df.groupby('complexity').agg({\n",
    "    'response_time_ms': ['mean', 'std'],\n",
    "    'tokens_used': ['mean', 'std'], \n",
    "    'sources_found': ['mean', 'std'],\n",
    "    'hallucination_score': 'mean',\n",
    "    'relevance_score': 'mean',\n",
    "    'qa_correctness_score': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"📊 Performance by Complexity Level:\")\n",
    "performance_by_complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization dashboard\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Response Time by Complexity',\n",
    "        'Token Usage by Complexity',\n",
    "        'Quality Scores by Complexity',\n",
    "        'Sources Found vs Expected',\n",
    "        'Quality Score Distribution',\n",
    "        'Performance vs Quality Correlation'\n",
    "    ],\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# Response time by complexity\n",
    "for complexity in phoenix_df['complexity'].unique():\n",
    "    data = phoenix_df[phoenix_df['complexity'] == complexity]['response_time_ms']\n",
    "    fig.add_trace(\n",
    "        go.Box(y=data, name=complexity, showlegend=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Token usage by complexity\n",
    "for complexity in phoenix_df['complexity'].unique():\n",
    "    data = phoenix_df[phoenix_df['complexity'] == complexity]['tokens_used']\n",
    "    fig.add_trace(\n",
    "        go.Box(y=data, name=complexity, showlegend=False),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Quality scores by complexity\n",
    "quality_cols = ['hallucination_score', 'relevance_score', 'qa_correctness_score']\n",
    "complexity_means = phoenix_df.groupby('complexity')[quality_cols].mean()\n",
    "\n",
    "for i, score_type in enumerate(quality_cols):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=complexity_means.index,\n",
    "            y=complexity_means[score_type],\n",
    "            name=score_type.replace('_score', ''),\n",
    "            showlegend=i==0\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Sources found vs expected\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=phoenix_df['expected_sources'],\n",
    "        y=phoenix_df['sources_found'],\n",
    "        mode='markers',\n",
    "        marker=dict(color=phoenix_df['complexity'].astype('category').cat.codes, colorscale='viridis'),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Add diagonal line for perfect source matching\n",
    "max_sources = max(phoenix_df['expected_sources'].max(), phoenix_df['sources_found'].max())\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, max_sources],\n",
    "        y=[0, max_sources],\n",
    "        mode='lines',\n",
    "        line=dict(dash='dash', color='red'),\n",
    "        name='Perfect Match',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Quality score distributions\n",
    "for score_type in quality_cols:\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=phoenix_df[score_type],\n",
    "            name=score_type.replace('_score', ''),\n",
    "            opacity=0.7,\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "# Performance vs Quality correlation\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=phoenix_df['response_time_ms'],\n",
    "        y=phoenix_df['qa_correctness_score'],\n",
    "        mode='markers',\n",
    "        marker=dict(size=phoenix_df['tokens_used']/50, opacity=0.6),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=3, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1200,\n",
    "    title_text=\"Multi-Agent System Evaluation Dashboard\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality analysis summary\n",
    "print(\"🎯 Quality Analysis Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n📈 Overall Performance:\")\n",
    "print(f\"  Average Hallucination Score: {phoenix_df['hallucination_score'].mean():.3f} (higher = better)\")\n",
    "print(f\"  Average Relevance Score: {phoenix_df['relevance_score'].mean():.3f} (higher = better)\")\n",
    "print(f\"  Average QA Correctness: {phoenix_df['qa_correctness_score'].mean():.3f} (higher = better)\")\n",
    "\n",
    "print(f\"\\n⚡ Performance Metrics:\")\n",
    "print(f\"  Average Response Time: {phoenix_df['response_time_ms'].mean():.0f}ms\")\n",
    "print(f\"  P95 Response Time: {phoenix_df['response_time_ms'].quantile(0.95):.0f}ms\")\n",
    "print(f\"  Average Token Usage: {phoenix_df['tokens_used'].mean():.0f}\")\n",
    "\n",
    "print(f\"\\n🎯 Accuracy Metrics:\")\n",
    "factual_responses = (phoenix_df['hallucination_label'] == 'factual').sum()\n",
    "relevant_responses = (phoenix_df['relevance_label'] == 'relevant').sum()\n",
    "correct_responses = (phoenix_df['qa_correctness_label'] == 'correct').sum()\n",
    "\n",
    "print(f\"  Factual Responses: {factual_responses}/{len(phoenix_df)} ({factual_responses/len(phoenix_df)*100:.1f}%)\")\n",
    "print(f\"  Relevant Responses: {relevant_responses}/{len(phoenix_df)} ({relevant_responses/len(phoenix_df)*100:.1f}%)\")\n",
    "print(f\"  Correct Responses: {correct_responses}/{len(phoenix_df)} ({correct_responses/len(phoenix_df)*100:.1f}%)\")\n",
    "\n",
    "# Performance targets from CLAUDE.md\n",
    "print(f\"\\n🎯 Target Achievement:\")\n",
    "p95_latency_simple = phoenix_df[phoenix_df['complexity'] == 'gpt-5-nano']['response_time_ms'].quantile(0.95)\n",
    "p95_latency_complex = phoenix_df[phoenix_df['complexity'] == 'gpt-5']['response_time_ms'].quantile(0.95)\n",
    "\n",
    "print(f\"  P95 Simple Query Latency: {p95_latency_simple:.0f}ms (Target: <3000ms) {'✅' if p95_latency_simple < 3000 else '❌'}\")\n",
    "print(f\"  P95 Complex Query Latency: {p95_latency_complex:.0f}ms (Target: <10000ms) {'✅' if p95_latency_complex < 10000 else '❌'}\")\n",
    "print(f\"  Overall QA Accuracy: {phoenix_df['qa_correctness_score'].mean()*100:.1f}% (Target: >90%) {'✅' if phoenix_df['qa_correctness_score'].mean() > 0.9 else '❌'}\")\n",
    "print(f\"  Factual Accuracy: {factual_responses/len(phoenix_df)*100:.1f}% (Target: >95%) {'✅' if factual_responses/len(phoenix_df) > 0.95 else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "final_results_path = 'comprehensive_agent_evaluation.csv'\n",
    "phoenix_df.to_csv(final_results_path, index=False)\n",
    "print(f\"💾 Comprehensive results saved to {final_results_path}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_stats = {\n",
    "    'total_queries': len(phoenix_df),\n",
    "    'avg_response_time_ms': phoenix_df['response_time_ms'].mean(),\n",
    "    'p95_response_time_ms': phoenix_df['response_time_ms'].quantile(0.95),\n",
    "    'avg_tokens_used': phoenix_df['tokens_used'].mean(),\n",
    "    'avg_hallucination_score': phoenix_df['hallucination_score'].mean(),\n",
    "    'avg_relevance_score': phoenix_df['relevance_score'].mean(),\n",
    "    'avg_qa_correctness_score': phoenix_df['qa_correctness_score'].mean(),\n",
    "    'factual_response_rate': factual_responses/len(phoenix_df),\n",
    "    'relevant_response_rate': relevant_responses/len(phoenix_df),\n",
    "    'correct_response_rate': correct_responses/len(phoenix_df),\n",
    "    'evaluation_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('evaluation_summary.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "    \n",
    "print(\"📋 Summary report saved to evaluation_summary.json\")\n",
    "print(\"\\n✅ Evaluation complete! Files saved:\")\n",
    "print(f\"  - {final_results_path} (detailed results)\")\n",
    "print(f\"  - evaluation_summary.json (summary metrics)\")\n",
    "print(f\"  - agent_evaluation_results.csv (raw agent responses)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "This notebook provides a comprehensive framework for testing and evaluating the multi-agent research system. Here's what you can do next:\n",
    "\n",
    "### 🔄 Continuous Evaluation\n",
    "- **Run this notebook regularly** as you develop and refine the agent system\n",
    "- **Compare results over time** to track improvements\n",
    "- **A/B test different agent configurations** using the same evaluation dataset\n",
    "\n",
    "### 📊 Analysis Extensions  \n",
    "- **Add custom evaluation metrics** specific to your use case\n",
    "- **Implement ground truth comparison** for more accurate quality assessment\n",
    "- **Add cost analysis** to track token usage and API costs\n",
    "\n",
    "### 🚀 Integration\n",
    "- **Connect to real agent system** by replacing mock_mode=False\n",
    "- **Set up automated evaluation pipeline** for CI/CD integration  \n",
    "- **Create alerting** for performance regressions\n",
    "\n",
    "### 📈 Phoenix Integration\n",
    "- **Upload results to Phoenix** for persistent monitoring\n",
    "- **Set up real-time evaluation** for production monitoring\n",
    "- **Create custom evaluators** for domain-specific quality metrics\n",
    "\n",
    "The evaluation framework is now ready to support the development and optimization of your multi-agent research system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}